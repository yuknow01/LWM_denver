{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02ced925-be5b-4bde-be12-01b379b794da",
   "metadata": {},
   "source": [
    "## dataset denver download if you download than pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ea5f9b0-cf35-48ec-aa33-dc418cd832b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: deepmimo in /home/dlghdbs200/.local/lib/python3.10/site-packages (4.0.0b10)\n",
      "Requirement already satisfied: scipy>=1.6.2 in /home/dlghdbs200/.local/lib/python3.10/site-packages (from deepmimo) (1.15.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from deepmimo) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.59.0 in /home/dlghdbs200/.local/lib/python3.10/site-packages (from deepmimo) (4.67.1)\n",
      "Requirement already satisfied: matplotlib>=3.8.2 in /home/dlghdbs200/.local/lib/python3.10/site-packages (from deepmimo) (3.10.3)\n",
      "Requirement already satisfied: numpy<2.3,>=1.19.5 in /home/dlghdbs200/.local/lib/python3.10/site-packages (from deepmimo) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.8.2->deepmimo) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib>=3.8.2->deepmimo) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/dlghdbs200/.local/lib/python3.10/site-packages (from matplotlib>=3.8.2->deepmimo) (0.12.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/dlghdbs200/.local/lib/python3.10/site-packages (from matplotlib>=3.8.2->deepmimo) (1.3.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/dlghdbs200/.local/lib/python3.10/site-packages (from matplotlib>=3.8.2->deepmimo) (1.4.8)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/dlghdbs200/.local/lib/python3.10/site-packages (from matplotlib>=3.8.2->deepmimo) (4.58.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.8.2->deepmimo) (2.9.0.post0)\n",
      "Requirement already satisfied: pillow>=8 in /home/dlghdbs200/.local/lib/python3.10/site-packages (from matplotlib>=3.8.2->deepmimo) (11.3.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->deepmimo) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->deepmimo) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->deepmimo) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->deepmimo) (2020.6.20)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib>=3.8.2->deepmimo) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install deepmimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33e19fd3-e6ec-4007-bb2f-2ce81ac1aa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepmimo as dm\n",
    "\n",
    "# Search for scenarios matching criteria\n",
    "scenarios = dm.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58402af8-16ae-47c0-9ed1-5b61727634e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario \"city_18_denver_28\" already exists in /home/dlghdbs200/LWM_denver/deepmimo_scenarios\n"
     ]
    }
   ],
   "source": [
    "import deepmimo as dm\n",
    "\n",
    "# Download a specific scenario\n",
    "dm.download('city_18_denver_28')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "213ec8a1-64cb-4102-a9b0-b9bbfb79addf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary, plot_summary\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(summary\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__file__\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from .summary import summary, plot_summary\n",
    "print(summary.__file__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7227ee19-0b5a-4452-ab65-23776cb56d83",
   "metadata": {},
   "source": [
    "# library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c343f294-aa27-4eb8-b6bf-44f358940851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import DeepMIMOv3\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import IterableDataset\n",
    "import numpy as np\n",
    "import time, gc\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from lwm_model import lwm\n",
    "from torch.optim import Adam\n",
    "from pathlib import Path\n",
    "import torch, time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c49d9ec5-08f0-4f80-a3d8-5e1700b9f56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbe816b-f8f0-42a4-a1db-26c752132b9b",
   "metadata": {},
   "source": [
    "# dataset load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9af341ca-03d3-423d-b26c-4169f2d34e3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TXRX PAIR: TXset 1 (tx_idx 0) & RXset 0 (rx_idxs 43248)\n",
      "Loading TXRX PAIR: TXset 2 (tx_idx 0) & RXset 0 (rx_idxs 43248)\n",
      "Loading TXRX PAIR: TXset 3 (tx_idx 0) & RXset 0 (rx_idxs 43248)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating channels: 100%|█████████████████████████████████████████████████████| 43248/43248 [00:01<00:00, 28438.00it/s]\n",
      "Generating channels: 100%|█████████████████████████████████████████████████████| 43248/43248 [00:01<00:00, 26584.81it/s]\n",
      "Generating channels: 100%|█████████████████████████████████████████████████████| 43248/43248 [00:01<00:00, 30158.47it/s]\n"
     ]
    }
   ],
   "source": [
    "import deepmimo as dm\n",
    "\n",
    "# Load dataset\n",
    "dataset = dm.load(\"city_18_denver_28\")\n",
    "\n",
    "# Access dataset properties\n",
    "aoa_az = dataset.aoa_az\n",
    "aoa_el = dataset.aoa_el\n",
    "inter_pos = dataset.inter_pos\n",
    "\n",
    "# Compute specific channel information\n",
    "# los = dataset.compute_los()\n",
    "channels = dataset.compute_channels()\n",
    "pl = dataset.compute_pathloss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bce33f9-9022-4773-bca7-ef7628975879",
   "metadata": {},
   "source": [
    "# Datashape\n",
    "deepmimo/generator/dataset.py def compute_channels in here channel data shape\n",
    "\n",
    "            numpy.ndarray: MIMO channel matrix with shape [n_users, n_rx_ant, n_tx_ant, n_subcarriers]\n",
    "                          if freq_domain=True, otherwise [n_users, n_rx_ant, n_tx_ant, n_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63259a57-d427-4106-b77a-70fc52058535",
   "metadata": {},
   "source": [
    "# three TxSet(1,2,3) \n",
    "“For 43,248 users, 1 user antenna, 8 BS antennas, and 1 propagation path.”\n",
    "(users, user antenna, BS antennas, PL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "823b2047-7527-4d61-bef6-102bafb4385d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43248, 1, 8, 1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channels[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c927c1fa-dad6-4581-ac78-bc2c65b19b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-4.0636469e-10-2.4687419e-10j],\n",
       "         [ 4.0486328e-10+2.4921543e-10j],\n",
       "         [-4.0334483e-10-2.5152960e-10j],\n",
       "         ...,\n",
       "         [ 3.9869291e-10+2.5830790e-10j],\n",
       "         [-3.9711204e-10-2.6051203e-10j],\n",
       "         [ 3.9551701e-10+2.6268823e-10j]]],\n",
       "\n",
       "\n",
       "       [[[ 1.5214810e-10-7.0644335e-10j],\n",
       "         [-1.5652508e-10+7.0226053e-10j],\n",
       "         [ 1.6082487e-10-6.9803024e-10j],\n",
       "         ...,\n",
       "         [-1.7325429e-10+6.8506756e-10j],\n",
       "         [ 1.7723865e-10-6.8066053e-10j],\n",
       "         [-1.8114261e-10+6.7621270e-10j]]],\n",
       "\n",
       "\n",
       "       [[[ 7.6910278e-10+1.6763185e-11j],\n",
       "         [-7.6921619e-10-1.8937532e-11j],\n",
       "         [ 7.6933526e-10+2.1130594e-11j],\n",
       "         ...,\n",
       "         [-7.6972168e-10-2.7824850e-11j],\n",
       "         [ 7.6985857e-10+3.0095502e-11j],\n",
       "         [-7.6999856e-10-3.2386184e-11j]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ 0.0000000e+00+0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j],\n",
       "         ...,\n",
       "         [ 0.0000000e+00+0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j]]],\n",
       "\n",
       "\n",
       "       [[[ 0.0000000e+00+0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j],\n",
       "         ...,\n",
       "         [ 0.0000000e+00+0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j]]],\n",
       "\n",
       "\n",
       "       [[[ 0.0000000e+00+0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j],\n",
       "         ...,\n",
       "         [ 0.0000000e+00+0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j]]]],\n",
       "      shape=(43248, 1, 8, 1), dtype=complex64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3e7a0995-f292-46b0-9f9c-1e11162cceb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(channels[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2832596c-936e-402e-b9db-70a75e79c58b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.3398023e-09-1.4365638e-09j], dtype=complex64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channels[1][2][0][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b5b8bf-d6c5-4584-9a92-5e6dbecf8827",
   "metadata": {},
   "source": [
    "# data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94d9cd7-da7a-4695-a61a-5ab7f4f0a58e",
   "metadata": {},
   "source": [
    "## How to predict \n",
    "H1,H2,H3 -> all data union <br>\n",
    "Tx_antenna split <br>\n",
    "Sliding-window version: Predicts the next antenna response from a few adjacent antennas (local spatial reconstruction).\n",
    "\n",
    "In summary, this code converts complex antenna channel data into real-valued, normalized tensors and prepares it for training.\n",
    "\n",
    "Three complex inputs (`H1`, `H2`, `H3`, each shaped `(N, 1, 8, 1)`) are merged into one dataset `(3N, 1, 8, 1)`. The real and imaginary parts are separated, forming `(3N, 1, 8, 1, 2)`. Each sample is normalized using its L2 norm.\n",
    "\n",
    "A sliding window is then applied along the antenna axis:\n",
    "\n",
    "* **Input:** 3 consecutive antennas (`win_in = 3`)\n",
    "* **Target:** the next antenna (`win_out = 1`)\n",
    "\n",
    "This produces 5 windows per sample (`(num_pos * 3N, 1, 3, 1, 2)` for inputs and `(num_pos * 3N, 1, 1, 1, 2)` for targets).\n",
    "\n",
    "Finally, data is split by antenna index:\n",
    "\n",
    "* **Training:** target index < 6\n",
    "* **Validation:** target index ≥ 6\n",
    "\n",
    "Both subsets are converted into PyTorch `DataLoader`s for efficient training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "22876e68-5361-4158-a252-df1ec766095d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windows per sample: 5\n",
      "Shapes -> X_win: torch.Size([648720, 1, 3, 1, 2]) Y_win: torch.Size([648720, 1, 1, 1, 2])\n",
      "Train/Val samples: 389232 / 259488\n",
      "Example shapes -> X: torch.Size([389232, 1, 3, 1, 2]), y: torch.Size([389232, 1, 1, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch, numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# ---------- 0) Load & combine three channels ----------\n",
    "H1, H2, H3 = channels[0], channels[1], channels[2]     # each: (N, 1, 8, 1), complex\n",
    "X = np.concatenate([H1, H2, H3], axis=0)               # (3N, 1, 8, 1)\n",
    "y = X.copy()\n",
    "\n",
    "# ---------- 1) Real/Imag split ----------\n",
    "X = np.stack([X.real, X.imag], axis=-1).astype(np.float32)  # (3N, 1, 8, 1, 2)\n",
    "y = np.stack([y.real, y.imag], axis=-1).astype(np.float32)\n",
    "\n",
    "# ---------- 2) Torch + per-sample normalization ----------\n",
    "X_t = torch.from_numpy(X).float()\n",
    "y_t = torch.from_numpy(y).float()\n",
    "eps = 1e-8\n",
    "scale = torch.linalg.vector_norm(X_t.view(X_t.size(0), -1), dim=1, keepdim=True).clamp_min(eps)\n",
    "X_t = X_t / scale.view(-1,1,1,1,1)\n",
    "y_t = y_t / scale.view(-1,1,1,1,1)\n",
    "\n",
    "# ---------- 3) Build antenna-axis sliding windows (3 in -> 1 out) ----------\n",
    "N3, _, n_tx, _, _ = X_t.shape     # n_tx should be 8 here\n",
    "assert n_tx >= 4, \"Need at least 4 antennas for 3->1 split.\"\n",
    "win_in, win_out = 3, 1\n",
    "num_pos = n_tx - (win_in + win_out) + 1      # 8 - 4 + 1 = 5\n",
    "\n",
    "X_chunks, Y_chunks, tgt_idx_list = [], [], []\n",
    "for p in range(num_pos):\n",
    "    X_chunks.append(X_t[:, :, p:p+win_in, :, :])                  # (3N,1,3,1,2)\n",
    "    Y_chunks.append(y_t[:, :, p+win_in:p+win_in+win_out, :, :])   # (3N,1,1,1,2)\n",
    "    tgt_idx_list.append(torch.full((N3,), p+win_in, dtype=torch.long))\n",
    "\n",
    "X_win = torch.cat(X_chunks, dim=0)     # (num_pos*3N, 1, 3, 1, 2)\n",
    "Y_win = torch.cat(Y_chunks, dim=0)     # (num_pos*3N, 1, 1, 1, 2)\n",
    "tgt_idx = torch.cat(tgt_idx_list, dim=0)  # (num_pos*3N,)\n",
    "print(\"Windows per sample:\", num_pos)\n",
    "print(\"Shapes -> X_win:\", X_win.shape, \"Y_win:\", Y_win.shape)\n",
    "\n",
    "# ---------- 4) Split by target-antenna: train(0..5), val(6..7) ----------\n",
    "train_ant = 6                              # target idx < 6 => train\n",
    "mask_tr = (tgt_idx < train_ant)\n",
    "mask_va = ~mask_tr\n",
    "\n",
    "X_tr, Y_tr = X_win[mask_tr], Y_win[mask_tr]\n",
    "X_va, Y_va = X_win[mask_va], Y_win[mask_va]\n",
    "\n",
    "# ---------- 5) DataLoaders (dataset carries only X,Y to avoid unpack errors) ----------\n",
    "train_set = TensorDataset(X_tr, Y_tr)\n",
    "val_set   = TensorDataset(X_va, Y_va)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=1024, shuffle=True)\n",
    "val_loader   = DataLoader(val_set,   batch_size=2048, shuffle=False)\n",
    "\n",
    "print(f\"Train/Val samples: {len(train_set)} / {len(val_set)}\")\n",
    "print(f\"Example shapes -> X: {X_tr.shape}, y: {Y_tr.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "72097380-6890-4db8-9d76-c99bddb28fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129744, 1, 8, 1, 2)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9729836e-c0b0-4a03-8483-1a59eb33432d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129744, 1, 8, 1, 2)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "48fc006b-a516-434d-967a-5f842642139f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0., 0.]],\n",
       " \n",
       "          [[0., 0.]],\n",
       " \n",
       "          [[0., 0.]]]]),\n",
       " tensor([[[[0., 0.]]]]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_set[12000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df1190e-f2d5-425a-85eb-e23ffaaa6d5f",
   "metadata": {},
   "source": [
    "# Model define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ecce1ee9-1398-4b28-b02c-38cc74a1df2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden=256, depth=3, pdrop=0.1):\n",
    "        super().__init__()\n",
    "        d_in, d_out = 3*2, 1*2  # (3 antennas * real+imag, 1 antenna * real+imag)\n",
    "        layers, d = [], d_in\n",
    "        for _ in range(depth - 1):\n",
    "            layers += [nn.Linear(d, hidden), nn.ReLU(), nn.Dropout(pdrop)]\n",
    "            d = hidden\n",
    "        layers += [nn.Linear(d, d_out)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b = x.size(0)\n",
    "        x = x.view(b, -1)        # (B, 6)\n",
    "        y = self.net(x)          # (B, 2)\n",
    "        return y.view(b, 1, 1, 1, 2)\n",
    "\n",
    "\n",
    "\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(2, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv1d(32, 64, 3, padding=1), nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv1d(64, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv1d(32, 2, 3, padding=1)\n",
    "        )\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)  # (B,2,3) -> (B,2,1)\n",
    "\n",
    "    def forward(self, x):                    # x: (B,1,3,1,2)\n",
    "        B, _, L, _, C = x.shape\n",
    "        assert L == 3 and C == 2, f\"expected (B,1,3,1,2), got {x.shape}\"\n",
    "        # (B,1,3,1,2) -> (B,3,1,2) -> (B,2,3)\n",
    "        x = x.squeeze(1).permute(0, 3, 1, 2).squeeze(-1)   # (B,2,3)\n",
    "        z = self.encoder(x)                                # (B,64,3)\n",
    "        z = self.decoder(z)                                # (B,2,3)\n",
    "        z = self.pool(z)                                   # (B,2,1)\n",
    "        return z.permute(0, 2, 1).unsqueeze(1).unsqueeze(3)  # (B,1,1,1,2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f4ae9def-0c2d-4758-84f7-2f209042dd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LWM_denver(nn.Module):\n",
    "    def __init__(self, d_model=64, n_layers=12, max_len=129):\n",
    "        super().__init__()\n",
    "        self.core = None\n",
    "        self.config = {\n",
    "            \"d_model\": d_model,\n",
    "            \"n_layers\": n_layers,\n",
    "            \"max_len\": max_len\n",
    "        }\n",
    "\n",
    "    def build_core(self, L, D, device):\n",
    "        from lwm_model import lwm\n",
    "        self.core = lwm(\n",
    "            element_length=D,\n",
    "            d_model=self.config[\"d_model\"],\n",
    "            n_layers=self.config[\"n_layers\"],\n",
    "            max_len=self.config[\"max_len\"]\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, _, L, _, D = x.shape   # (B,1,3,1,2)\n",
    "        x_seq = x.squeeze(1).squeeze(-2).float()  # (B,3,2)\n",
    "        if self.core is None:\n",
    "            self.build_core(L, D, x.device)\n",
    "        # forward\n",
    "        y_hat, _ = self.core(x_seq, torch.arange(L, device=x.device).unsqueeze(0).repeat(B, 1))\n",
    "        # Take last token output only (predict next antenna)\n",
    "        y_last = y_hat[:, -1:, :]  # (B,1,2)\n",
    "        return y_last.unsqueeze(1).unsqueeze(-2)  # (B,1,1,1,2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87433429-9477-4b8f-a1dd-35619b586409",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9229e792-95b4-48a0-b1de-1a579632ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x)\n",
    "        loss = criterion(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, total_mse, total_nmse = 0.0, 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y)\n",
    "            mse = torch.mean((pred - y) ** 2).item()\n",
    "            nmse = mse / torch.mean(y ** 2).item()\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            total_mse += mse * x.size(0)\n",
    "            total_nmse += nmse * x.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    avg_rmse = (total_mse / len(dataloader.dataset)) ** 0.5\n",
    "    avg_nmse = total_nmse / len(dataloader.dataset)\n",
    "    avg_nmse_db = 10 * np.log10(avg_nmse + 1e-12)\n",
    "    return avg_loss, avg_rmse, avg_nmse, avg_nmse_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "991e24de-b922-4932-a2b4-4b541cf7e750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model, epochs=150, lr=1e-3, weight_decay=0.0, show_every=1):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss(reduction=\"mean\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    best_nmse = float(\"inf\")\n",
    "    best_rmse = float(\"inf\")\n",
    "    best_nmse_db = float(\"inf\")\n",
    "\n",
    "    best_epoch_loss = 0\n",
    "    best_epoch_nmse = 0\n",
    "    best_epoch_rmse = 0\n",
    "    best_epoch_nmse_db = 0\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        t0 = time.time()\n",
    "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_loss, rmse, nmse, nmse_db = evaluate(model, val_loader, criterion)\n",
    "        dt = time.time() - t0\n",
    "\n",
    "        # Track best values\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_epoch_loss = ep\n",
    "\n",
    "        if nmse < best_nmse:\n",
    "            best_nmse = nmse\n",
    "            best_epoch_nmse = ep\n",
    "\n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_epoch_rmse = ep\n",
    "\n",
    "        if nmse_db < best_nmse_db:\n",
    "            best_nmse_db = nmse_db\n",
    "            best_epoch_nmse_db = ep\n",
    "\n",
    "        # Logging\n",
    "        if ep % show_every == 0:\n",
    "            print(f\"[{ep:02d}/{epochs:03d}] \"\n",
    "                  f\"TrainLoss: {train_loss:.6f}  \"\n",
    "                  f\"ValLoss: {val_loss:.6f}  \"\n",
    "                  f\"Val RMSE: {rmse:.4f}  \"\n",
    "                  f\"Val NMSE: {nmse:.4f}  \"\n",
    "                  f\"Val NMSE_dB: {nmse_db:.6f} dB  \"\n",
    "                  f\"TrainTime: {dt:.2f}s\")\n",
    "\n",
    "    # Summary of best metrics\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"=> Best ValLoss   : {best_loss:.6f}   (epoch {best_epoch_loss})\")\n",
    "    print(f\"=> Best Val NMSE  : {best_nmse:.4f}   (epoch {best_epoch_nmse})\")\n",
    "    print(f\"=> Best Val RMSE  : {best_rmse:.4f}   (epoch {best_epoch_rmse})\")\n",
    "    print(f\"=> Best Val NMSE_dB: {best_nmse_db:.4f} dB (epoch {best_epoch_nmse_db})\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "95d40ca9-e4ef-4397-9d25-9730d902a42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MLP Training ===\n",
      "=== Training MLP ===\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mredirect_stdout(tee):\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Training MLP ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m         \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMLP training log saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmlp_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== CNN Training ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[49], line 2\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(model, epochs, lr, weight_decay, show_every)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(model, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, show_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss(reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39mweight_decay)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1355\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1353\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:942\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 942\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    943\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    945\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1341\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1336\u001b[0m             device,\n\u001b[1;32m   1337\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1338\u001b[0m             non_blocking,\n\u001b[1;32m   1339\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1340\u001b[0m         )\n\u001b[0;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import sys, os, contextlib\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "epochs = 150\n",
    "\n",
    "# --- Tee that mirrors stdout to both console and a file ---\n",
    "class Tee:\n",
    "    def __init__(self, filename, mode=\"w\", encoding=\"utf-8\"):\n",
    "        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "        self.file = open(filename, mode, encoding=encoding)\n",
    "        self.stdout = sys.stdout\n",
    "\n",
    "    def write(self, data):\n",
    "        self.file.write(data)\n",
    "        self.stdout.write(data)\n",
    "\n",
    "    def flush(self):\n",
    "        self.file.flush()\n",
    "        self.stdout.flush()\n",
    "\n",
    "    def close(self):\n",
    "        self.file.close()\n",
    "\n",
    "    # context manager support\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc, tb):\n",
    "        self.close()\n",
    "        # don't suppress exceptions\n",
    "        return False\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# \n",
    "base_dir = \"/home/dlghdbs200/LWM_denver\"\n",
    "\n",
    "print(\"=== MLP Training ===\")\n",
    "mlp = MLP(hidden=256, depth=3, pdrop=0.1)\n",
    "mlp_path = os.path.join(base_dir, \"MLP_epoch500_antenna.txt\")\n",
    "with Tee(mlp_path, \"w\") as tee:\n",
    "    with contextlib.redirect_stdout(tee):\n",
    "        print(\"=== Training MLP ===\")\n",
    "        run(mlp, epochs=1, lr=1e-3)\n",
    "print(f\"\\nMLP training log saved to {mlp_path}\")\n",
    "\n",
    "print(\"\\n=== CNN Training ===\")\n",
    "cnn = CNN1D()\n",
    "cnn_path = os.path.join(base_dir, \"CNN_epoch500_antenna.txt\")\n",
    "with Tee(cnn_path, \"w\") as tee:\n",
    "    with contextlib.redirect_stdout(tee):\n",
    "        print(\"=== Training CNN ===\")\n",
    "        run(cnn, epochs=1, lr=1e-3)\n",
    "print(f\"\\nCNN training log saved to {cnn_path}\")\n",
    "\n",
    "print(\"\\n=== LWM Training ===\")\n",
    "\n",
    "lwm_model = LWM_denver(d_model=64, n_layers=12)\n",
    "lwm_path = os.path.join(base_dir, \"LWM_epoch150_antenna.txt\")\n",
    "with Tee(lwm_path, \"w\") as tee:\n",
    "    with contextlib.redirect_stdout(tee):\n",
    "        print(\"=== Training LWM ===\")\n",
    "        sample_x, _ = next(iter(train_loader))\n",
    "        L = sample_x.shape[2]\n",
    "        D = sample_x.shape[-1]\n",
    "        lwm_model.build_core(L=L, D=D, device=device)\n",
    "        run(lwm_model, epochs=epochs, lr=1e-3)\n",
    "print(f\"\\nLWM training log saved to {lwm_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bd94bb-9867-4fb0-bbf5-0ac7bc61640d",
   "metadata": {},
   "source": [
    "# inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0010989b-a534-47a5-851a-c3c20c8c2ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a8b00c-71e5-4359-b889-1ad2756396cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc61b3d-3552-45fb-9f6d-c34c1d668226",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
