{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02ced925-be5b-4bde-be12-01b379b794da",
   "metadata": {},
   "source": [
    "## dataset denver download if you download than pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33e19fd3-e6ec-4007-bb2f-2ce81ac1aa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepmimo as dm\n",
    "\n",
    "# Search for scenarios matching criteria\n",
    "scenarios = dm.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58402af8-16ae-47c0-9ed1-5b61727634e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario \"city_18_denver_28\" already exists in /home/dlghdbs200/LWM_denver/deepmimo_scenarios\n"
     ]
    }
   ],
   "source": [
    "import deepmimo as dm\n",
    "\n",
    "# Download a specific scenario\n",
    "dm.download('city_18_denver_28')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7227ee19-0b5a-4452-ab65-23776cb56d83",
   "metadata": {},
   "source": [
    "# library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c343f294-aa27-4eb8-b6bf-44f358940851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import DeepMIMOv3\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import IterableDataset\n",
    "import numpy as np\n",
    "import time, gc\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from lwm_model import lwm\n",
    "from torch.optim import Adam\n",
    "from pathlib import Path\n",
    "import torch, time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbe816b-f8f0-42a4-a1db-26c752132b9b",
   "metadata": {},
   "source": [
    "# dataset load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9af341ca-03d3-423d-b26c-4169f2d34e3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TXRX PAIR: TXset 1 (tx_idx 0) & RXset 0 (rx_idxs 43248)\n",
      "Loading TXRX PAIR: TXset 2 (tx_idx 0) & RXset 0 (rx_idxs 43248)\n",
      "Loading TXRX PAIR: TXset 3 (tx_idx 0) & RXset 0 (rx_idxs 43248)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating channels: 100%|█████████████████████████████████████████████████████| 43248/43248 [00:01<00:00, 26695.82it/s]\n",
      "Generating channels: 100%|█████████████████████████████████████████████████████| 43248/43248 [00:01<00:00, 24979.71it/s]\n",
      "Generating channels: 100%|█████████████████████████████████████████████████████| 43248/43248 [00:01<00:00, 28834.32it/s]\n"
     ]
    }
   ],
   "source": [
    "import deepmimo as dm\n",
    "\n",
    "# Load dataset\n",
    "dataset = dm.load(\"city_18_denver_28\")\n",
    "\n",
    "# Access dataset properties\n",
    "aoa_az = dataset.aoa_az\n",
    "aoa_el = dataset.aoa_el\n",
    "inter_pos = dataset.inter_pos\n",
    "\n",
    "# Compute specific channel information\n",
    "# los = dataset.compute_los()\n",
    "channels = dataset.compute_channels()\n",
    "pl = dataset.compute_pathloss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bce33f9-9022-4773-bca7-ef7628975879",
   "metadata": {},
   "source": [
    "# Datashape\n",
    "deepmimo/generator/dataset.py def compute_channels in here channel data shape\n",
    "\n",
    "            numpy.ndarray: MIMO channel matrix with shape [n_users, n_rx_ant, n_tx_ant, n_subcarriers]\n",
    "                          if freq_domain=True, otherwise [n_users, n_rx_ant, n_tx_ant, n_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63259a57-d427-4106-b77a-70fc52058535",
   "metadata": {},
   "source": [
    "# three TxSet(1,2,3) \n",
    "“For 43,248 users, 1 user antenna, 8 BS antennas, and 1 propagation path.”\n",
    "(users, user antenna, BS antennas, PL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "823b2047-7527-4d61-bef6-102bafb4385d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43248, 1, 8, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channels[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c927c1fa-dad6-4581-ac78-bc2c65b19b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-4.0636469e-10-2.4687419e-10j],\n",
       "         [ 4.0486328e-10+2.4921543e-10j],\n",
       "         [-4.0334483e-10-2.5152960e-10j],\n",
       "         ...,\n",
       "         [ 3.9869291e-10+2.5830790e-10j],\n",
       "         [-3.9711204e-10-2.6051203e-10j],\n",
       "         [ 3.9551701e-10+2.6268823e-10j]]],\n",
       "\n",
       "\n",
       "       [[[ 1.5214810e-10-7.0644335e-10j],\n",
       "         [-1.5652508e-10+7.0226053e-10j],\n",
       "         [ 1.6082487e-10-6.9803024e-10j],\n",
       "         ...,\n",
       "         [-1.7325429e-10+6.8506756e-10j],\n",
       "         [ 1.7723865e-10-6.8066053e-10j],\n",
       "         [-1.8114261e-10+6.7621270e-10j]]],\n",
       "\n",
       "\n",
       "       [[[ 7.6910278e-10+1.6763185e-11j],\n",
       "         [-7.6921619e-10-1.8937532e-11j],\n",
       "         [ 7.6933526e-10+2.1130594e-11j],\n",
       "         ...,\n",
       "         [-7.6972168e-10-2.7824850e-11j],\n",
       "         [ 7.6985857e-10+3.0095502e-11j],\n",
       "         [-7.6999856e-10-3.2386184e-11j]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ 0.0000000e+00+0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j],\n",
       "         ...,\n",
       "         [ 0.0000000e+00+0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j]]],\n",
       "\n",
       "\n",
       "       [[[ 0.0000000e+00+0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j],\n",
       "         ...,\n",
       "         [ 0.0000000e+00+0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j]]],\n",
       "\n",
       "\n",
       "       [[[ 0.0000000e+00+0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j],\n",
       "         ...,\n",
       "         [ 0.0000000e+00+0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j]]]],\n",
       "      shape=(43248, 1, 8, 1), dtype=complex64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e7a0995-f292-46b0-9f9c-1e11162cceb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.0468159e-10-5.8945265e-10j],\n",
       "       [ 2.3423355e-10+4.7612841e-10j],\n",
       "       [-2.8508820e-10-3.6341480e-10j],\n",
       "       [ 4.1828480e-10+3.4321815e-10j],\n",
       "       [-5.3884103e-10-4.4782919e-10j],\n",
       "       [ 5.5945587e-10+6.2805278e-10j],\n",
       "       [-4.5894541e-10-7.8667256e-10j],\n",
       "       [ 2.9558433e-10+8.4439894e-10j]], dtype=complex64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channels[1][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b5b8bf-d6c5-4584-9a92-5e6dbecf8827",
   "metadata": {},
   "source": [
    "# data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94d9cd7-da7a-4695-a61a-5ab7f4f0a58e",
   "metadata": {},
   "source": [
    "## How to predict \n",
    "H1,H2,H3 -> all data union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22876e68-5361-4158-a252-df1ec766095d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 97308, Val samples: 32436\n"
     ]
    }
   ],
   "source": [
    "H1, H2, H3 = channels[0], channels[1], channels[2]\n",
    "X = np.concatenate([H1,H2,H3], axis = 0)\n",
    "y = X.copy()\n",
    "\n",
    "# 1) Numpy -> Tensor (+ float32) & (optional) per-sample normalization\n",
    "X_t = torch.from_numpy(X).float()\n",
    "y_t = torch.from_numpy(y).float()\n",
    "\n",
    "eps = 1e-8\n",
    "# L2 norm \n",
    "scale = torch.linalg.vector_norm(X_t.view(X_t.size(0), -1), dim=1, keepdim=True).clamp_min(eps)\n",
    "X_t = X_t / scale.view(-1,1,1,1)\n",
    "y_t = y_t / scale.view(-1,1,1,1)\n",
    "\n",
    "# 2) Split only by users (inputs=H1,H2; label=H3)\n",
    "dataset = TensorDataset(X_t, y_t)\n",
    "N = len(dataset)\n",
    "n_tr = int(N*0.75)         # e.g., 75% train, 25% val\n",
    "n_val = N - n_tr\n",
    "\n",
    "# random seed 42 fixed\n",
    "train_set, val_set = random_split(dataset, [n_tr, n_val], generator=torch.Generator().manual_seed(42))\n",
    "train_loader = DataLoader(train_set, batch_size=1024, shuffle=True)\n",
    "val_loader   = DataLoader(val_set,   batch_size=2048, shuffle=False)\n",
    "\n",
    "print(f\"Train samples: {len(train_set)}, Val samples: {len(val_set)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72097380-6890-4db8-9d76-c99bddb28fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129744, 1, 8, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9729836e-c0b0-4a03-8483-1a59eb33432d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129744, 1, 8, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df1190e-f2d5-425a-85eb-e23ffaaa6d5f",
   "metadata": {},
   "source": [
    "# Model define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ecce1ee9-1398-4b28-b02c-38cc74a1df2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Models (same interface as your run())\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden=256, depth=3, pdrop=0.1):\n",
    "        super().__init__()\n",
    "        layers, d = [], 8\n",
    "        for _ in range(depth-1):\n",
    "            layers += [nn.Linear(d, hidden), nn.ReLU(), nn.Dropout(pdrop)]\n",
    "            d = hidden\n",
    "        layers += [nn.Linear(d, 8)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x):                 # x: (B,1,8,1)\n",
    "        x = x.view(x.size(0), -1)         # (B,8)\n",
    "        y = self.net(x)                   # (B,8)\n",
    "        return y.view(-1,1,8,1)           # (B,1,8,1)\n",
    "\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feat = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv1d(32, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv1d(64, 64, 3, padding=1), nn.ReLU(),\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),  # (B,64,8) -> (B,64,1)\n",
    "            nn.Flatten(),             # (B,64)\n",
    "            nn.Linear(64, 64), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 8)\n",
    "        )\n",
    "    def forward(self, x):                 # x: (B,1,8,1)\n",
    "        x = x.squeeze(-1)                 # (B,1,8)\n",
    "        h = self.feat(x)                  # (B,64,8)\n",
    "        y = self.head(h)                  # (B,8)\n",
    "        return y.view(-1,1,8,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f4ae9def-0c2d-4758-84f7-2f209042dd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LWM_denver(nn.Module):\n",
    "    def __init__(self, d_model=64, n_layers=12, max_len=129):\n",
    "        super().__init__()\n",
    "        # The actual LWM model (Transformer) will be built later (lazy initialization)\n",
    "        self.core = None\n",
    "\n",
    "        # Store configuration values for later use\n",
    "        # These will be used when building the LWM model dynamically\n",
    "        self.config = {\n",
    "            \"d_model\": d_model,\n",
    "            \"n_layers\": n_layers,\n",
    "            \"max_len\": max_len\n",
    "        }\n",
    "\n",
    "    def build_core(self, L, D, device):\n",
    "        \"\"\"\n",
    "        Build the actual LWM model (\"core\") dynamically when the first input is received.\n",
    "        - L: sequence length (usually equal to the number of Tx antennas)\n",
    "        - D: feature dimension per token\n",
    "        - device: which device (CPU/GPU) to place the model on\n",
    "        \"\"\"\n",
    "       \n",
    "        # Create the LWM model (Transformer)\n",
    "        self.core = lwm(\n",
    "            element_length=D,                   # Input feature dimension\n",
    "            d_model=self.config[\"d_model\"],     # Hidden size of the Transformer\n",
    "            max_len=self.config[\"max_len\"],     # Maximum positional embedding length\n",
    "            n_layers=self.config[\"n_layers\"]    # Number of encoder layers\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the model.\n",
    "\n",
    "        Input tensor x shape:\n",
    "            [B, n_rx_ant, n_tx_ant, n_paths]\n",
    "        Example: [batch_size, 1, 8, 1]\n",
    "\n",
    "        Steps:\n",
    "        1. Extract B and C (C = number of Tx antennas)\n",
    "        2. Convert 4D input -> 3D sequence [B, L, D]\n",
    "        3. Build the core model if not already created\n",
    "        4. Feed the input into LWM (self.core)\n",
    "        5. Reconstruct back to 4D output shape\n",
    "        \"\"\"\n",
    "        \n",
    "        B, _, C, _ = x.shape      # B: batch size, C: number of Tx antennas\n",
    "        L, D = C, 1               # L: sequence length, D: feature dimension (1)\n",
    "        x_seq = x.squeeze(1).squeeze(-1).unsqueeze(-1).float()  # (B,1,C,1) → (B,L,D)\n",
    "\n",
    "        # Build the core model only once (lazy initialization)\n",
    "        if self.core is None:\n",
    "            self.build_core(L, D, x.device)\n",
    "\n",
    "        # Generate mask positions for full-sequence prediction\n",
    "        masked_pos = torch.arange(L, device=x.device).unsqueeze(0).repeat(B, 1)  # [B, L]\n",
    "\n",
    "        # Forward pass through the LWM model\n",
    "        y_hat, _ = self.core(x_seq, masked_pos)  # Output shape: [B, L, D]\n",
    "\n",
    "        # Reshape back to 4D for consistency with MLP/CNN outputs\n",
    "        return y_hat.unsqueeze(1)  # [B, 1, L, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87433429-9477-4b8f-a1dd-35619b586409",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9229e792-95b4-48a0-b1de-1a579632ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x)\n",
    "        loss = criterion(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, total_mse, total_nmse = 0.0, 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y)\n",
    "            mse = torch.mean((pred - y) ** 2).item()\n",
    "            nmse = mse / torch.mean(y ** 2).item()\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            total_mse += mse * x.size(0)\n",
    "            total_nmse += nmse * x.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    avg_rmse = (total_mse / len(dataloader.dataset)) ** 0.5\n",
    "    avg_nmse = total_nmse / len(dataloader.dataset)\n",
    "    avg_nmse_db = 10 * np.log10(avg_nmse + 1e-12)\n",
    "    return avg_loss, avg_rmse, avg_nmse, avg_nmse_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "991e24de-b922-4932-a2b4-4b541cf7e750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model, epochs=150, lr=1e-3, weight_decay=0.0, show_every=1):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss(reduction=\"mean\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    best_nmse = float(\"inf\")\n",
    "    best_rmse = float(\"inf\")\n",
    "    best_nmse_db = float(\"inf\")\n",
    "\n",
    "    best_epoch_loss = 0\n",
    "    best_epoch_nmse = 0\n",
    "    best_epoch_rmse = 0\n",
    "    best_epoch_nmse_db = 0\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        t0 = time.time()\n",
    "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_loss, rmse, nmse, nmse_db = evaluate(model, val_loader, criterion)\n",
    "        dt = time.time() - t0\n",
    "\n",
    "        # Track best values\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_epoch_loss = ep\n",
    "\n",
    "        if nmse < best_nmse:\n",
    "            best_nmse = nmse\n",
    "            best_epoch_nmse = ep\n",
    "\n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_epoch_rmse = ep\n",
    "\n",
    "        if nmse_db < best_nmse_db:\n",
    "            best_nmse_db = nmse_db\n",
    "            best_epoch_nmse_db = ep\n",
    "\n",
    "        # Logging\n",
    "        if ep % show_every == 0:\n",
    "            print(f\"[{ep:02d}/{epochs:03d}] \"\n",
    "                  f\"TrainLoss: {train_loss:.6f}  \"\n",
    "                  f\"ValLoss: {val_loss:.6f}  \"\n",
    "                  f\"Val RMSE: {rmse:.4f}  \"\n",
    "                  f\"Val NMSE: {nmse:.4f}  \"\n",
    "                  f\"Val NMSE_dB: {nmse_db:.6f} dB  \"\n",
    "                  f\"TrainTime: {dt:.2f}s\")\n",
    "\n",
    "    # Summary of best metrics\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"=> Best ValLoss   : {best_loss:.6f}   (epoch {best_epoch_loss})\")\n",
    "    print(f\"=> Best Val NMSE  : {best_nmse:.4f}   (epoch {best_epoch_nmse})\")\n",
    "    print(f\"=> Best Val RMSE  : {best_rmse:.4f}   (epoch {best_epoch_rmse})\")\n",
    "    print(f\"=> Best Val NMSE_dB: {best_nmse_db:.4f} dB (epoch {best_epoch_nmse_db})\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "95d40ca9-e4ef-4397-9d25-9730d902a42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MLP Training ===\n",
      "=== Training MLP ===\n",
      "[01/050] TrainLoss: 0.004490  ValLoss: 0.000186  Val RMSE: 0.0136  Val NMSE: 0.0037  Val NMSE_dB: -24.270139 dB  TrainTime: 1.26s\n",
      "[02/050] TrainLoss: 0.001060  ValLoss: 0.000137  Val RMSE: 0.0117  Val NMSE: 0.0028  Val NMSE_dB: -25.586533 dB  TrainTime: 1.17s\n",
      "[03/050] TrainLoss: 0.000925  ValLoss: 0.000108  Val RMSE: 0.0104  Val NMSE: 0.0022  Val NMSE_dB: -26.623523 dB  TrainTime: 1.12s\n",
      "[04/050] TrainLoss: 0.000866  ValLoss: 0.000145  Val RMSE: 0.0121  Val NMSE: 0.0029  Val NMSE_dB: -25.333675 dB  TrainTime: 1.18s\n",
      "[05/050] TrainLoss: 0.000828  ValLoss: 0.000156  Val RMSE: 0.0125  Val NMSE: 0.0031  Val NMSE_dB: -25.037170 dB  TrainTime: 1.16s\n",
      "[06/050] TrainLoss: 0.000797  ValLoss: 0.000203  Val RMSE: 0.0142  Val NMSE: 0.0041  Val NMSE_dB: -23.897226 dB  TrainTime: 1.29s\n",
      "[07/050] TrainLoss: 0.000788  ValLoss: 0.000278  Val RMSE: 0.0167  Val NMSE: 0.0056  Val NMSE_dB: -22.526436 dB  TrainTime: 1.23s\n",
      "[08/050] TrainLoss: 0.000775  ValLoss: 0.000164  Val RMSE: 0.0128  Val NMSE: 0.0033  Val NMSE_dB: -24.802514 dB  TrainTime: 1.17s\n",
      "[09/050] TrainLoss: 0.000744  ValLoss: 0.000119  Val RMSE: 0.0109  Val NMSE: 0.0024  Val NMSE_dB: -26.188259 dB  TrainTime: 1.12s\n",
      "[10/050] TrainLoss: 0.000733  ValLoss: 0.000149  Val RMSE: 0.0122  Val NMSE: 0.0030  Val NMSE_dB: -25.216599 dB  TrainTime: 1.20s\n",
      "[11/050] TrainLoss: 0.000737  ValLoss: 0.000105  Val RMSE: 0.0103  Val NMSE: 0.0021  Val NMSE_dB: -26.742031 dB  TrainTime: 1.31s\n",
      "[12/050] TrainLoss: 0.000721  ValLoss: 0.000106  Val RMSE: 0.0103  Val NMSE: 0.0021  Val NMSE_dB: -26.692468 dB  TrainTime: 1.09s\n",
      "[13/050] TrainLoss: 0.000710  ValLoss: 0.000154  Val RMSE: 0.0124  Val NMSE: 0.0031  Val NMSE_dB: -25.085652 dB  TrainTime: 1.24s\n",
      "[14/050] TrainLoss: 0.000713  ValLoss: 0.000141  Val RMSE: 0.0119  Val NMSE: 0.0028  Val NMSE_dB: -25.460848 dB  TrainTime: 1.24s\n",
      "[15/050] TrainLoss: 0.000700  ValLoss: 0.000157  Val RMSE: 0.0125  Val NMSE: 0.0032  Val NMSE_dB: -24.993606 dB  TrainTime: 1.12s\n",
      "[16/050] TrainLoss: 0.000698  ValLoss: 0.000188  Val RMSE: 0.0137  Val NMSE: 0.0038  Val NMSE_dB: -24.219227 dB  TrainTime: 1.13s\n",
      "[17/050] TrainLoss: 0.000693  ValLoss: 0.000116  Val RMSE: 0.0108  Val NMSE: 0.0023  Val NMSE_dB: -26.320657 dB  TrainTime: 1.01s\n",
      "[18/050] TrainLoss: 0.000686  ValLoss: 0.000140  Val RMSE: 0.0118  Val NMSE: 0.0028  Val NMSE_dB: -25.500358 dB  TrainTime: 1.19s\n",
      "[19/050] TrainLoss: 0.000689  ValLoss: 0.000233  Val RMSE: 0.0153  Val NMSE: 0.0047  Val NMSE_dB: -23.286431 dB  TrainTime: 1.17s\n",
      "[20/050] TrainLoss: 0.000686  ValLoss: 0.000182  Val RMSE: 0.0135  Val NMSE: 0.0037  Val NMSE_dB: -24.351618 dB  TrainTime: 1.20s\n",
      "[21/050] TrainLoss: 0.000682  ValLoss: 0.000168  Val RMSE: 0.0129  Val NMSE: 0.0034  Val NMSE_dB: -24.717828 dB  TrainTime: 0.93s\n",
      "[22/050] TrainLoss: 0.000675  ValLoss: 0.000146  Val RMSE: 0.0121  Val NMSE: 0.0029  Val NMSE_dB: -25.312905 dB  TrainTime: 0.92s\n",
      "[23/050] TrainLoss: 0.000671  ValLoss: 0.000165  Val RMSE: 0.0129  Val NMSE: 0.0033  Val NMSE_dB: -24.777754 dB  TrainTime: 0.96s\n",
      "[24/050] TrainLoss: 0.000676  ValLoss: 0.000198  Val RMSE: 0.0141  Val NMSE: 0.0040  Val NMSE_dB: -23.993359 dB  TrainTime: 1.02s\n",
      "[25/050] TrainLoss: 0.000663  ValLoss: 0.000098  Val RMSE: 0.0099  Val NMSE: 0.0020  Val NMSE_dB: -27.054498 dB  TrainTime: 1.15s\n",
      "[26/050] TrainLoss: 0.000660  ValLoss: 0.000226  Val RMSE: 0.0150  Val NMSE: 0.0046  Val NMSE_dB: -23.414321 dB  TrainTime: 1.03s\n",
      "[27/050] TrainLoss: 0.000666  ValLoss: 0.000104  Val RMSE: 0.0102  Val NMSE: 0.0021  Val NMSE_dB: -26.810458 dB  TrainTime: 1.13s\n",
      "[28/050] TrainLoss: 0.000661  ValLoss: 0.000115  Val RMSE: 0.0107  Val NMSE: 0.0023  Val NMSE_dB: -26.355445 dB  TrainTime: 1.20s\n",
      "[29/050] TrainLoss: 0.000664  ValLoss: 0.000215  Val RMSE: 0.0147  Val NMSE: 0.0043  Val NMSE_dB: -23.639694 dB  TrainTime: 1.22s\n",
      "[30/050] TrainLoss: 0.000662  ValLoss: 0.000122  Val RMSE: 0.0110  Val NMSE: 0.0024  Val NMSE_dB: -26.114645 dB  TrainTime: 0.20s\n",
      "[31/050] TrainLoss: 0.000658  ValLoss: 0.000285  Val RMSE: 0.0169  Val NMSE: 0.0057  Val NMSE_dB: -22.408355 dB  TrainTime: 1.16s\n",
      "[32/050] TrainLoss: 0.000684  ValLoss: 0.000211  Val RMSE: 0.0145  Val NMSE: 0.0042  Val NMSE_dB: -23.716852 dB  TrainTime: 1.10s\n",
      "[33/050] TrainLoss: 0.000671  ValLoss: 0.000185  Val RMSE: 0.0136  Val NMSE: 0.0037  Val NMSE_dB: -24.292867 dB  TrainTime: 1.21s\n",
      "[34/050] TrainLoss: 0.000654  ValLoss: 0.000214  Val RMSE: 0.0146  Val NMSE: 0.0043  Val NMSE_dB: -23.653852 dB  TrainTime: 1.17s\n",
      "[35/050] TrainLoss: 0.000662  ValLoss: 0.000185  Val RMSE: 0.0136  Val NMSE: 0.0037  Val NMSE_dB: -24.298683 dB  TrainTime: 0.97s\n",
      "[36/050] TrainLoss: 0.000651  ValLoss: 0.000099  Val RMSE: 0.0100  Val NMSE: 0.0020  Val NMSE_dB: -26.988788 dB  TrainTime: 0.97s\n",
      "[37/050] TrainLoss: 0.000651  ValLoss: 0.000208  Val RMSE: 0.0144  Val NMSE: 0.0042  Val NMSE_dB: -23.791209 dB  TrainTime: 1.01s\n",
      "[38/050] TrainLoss: 0.000658  ValLoss: 0.000155  Val RMSE: 0.0125  Val NMSE: 0.0031  Val NMSE_dB: -25.055014 dB  TrainTime: 0.96s\n",
      "[39/050] TrainLoss: 0.000657  ValLoss: 0.000200  Val RMSE: 0.0141  Val NMSE: 0.0040  Val NMSE_dB: -23.960579 dB  TrainTime: 1.14s\n",
      "[40/050] TrainLoss: 0.000654  ValLoss: 0.000197  Val RMSE: 0.0140  Val NMSE: 0.0040  Val NMSE_dB: -24.011190 dB  TrainTime: 1.26s\n",
      "[41/050] TrainLoss: 0.000658  ValLoss: 0.000175  Val RMSE: 0.0132  Val NMSE: 0.0035  Val NMSE_dB: -24.521883 dB  TrainTime: 1.06s\n",
      "[42/050] TrainLoss: 0.000651  ValLoss: 0.000247  Val RMSE: 0.0157  Val NMSE: 0.0050  Val NMSE_dB: -23.032984 dB  TrainTime: 1.13s\n",
      "[43/050] TrainLoss: 0.000657  ValLoss: 0.000173  Val RMSE: 0.0132  Val NMSE: 0.0035  Val NMSE_dB: -24.579678 dB  TrainTime: 1.09s\n",
      "[44/050] TrainLoss: 0.000648  ValLoss: 0.000195  Val RMSE: 0.0140  Val NMSE: 0.0039  Val NMSE_dB: -24.064492 dB  TrainTime: 1.11s\n",
      "[45/050] TrainLoss: 0.000645  ValLoss: 0.000139  Val RMSE: 0.0118  Val NMSE: 0.0028  Val NMSE_dB: -25.545527 dB  TrainTime: 1.18s\n",
      "[46/050] TrainLoss: 0.000634  ValLoss: 0.000170  Val RMSE: 0.0130  Val NMSE: 0.0034  Val NMSE_dB: -24.668592 dB  TrainTime: 1.13s\n",
      "[47/050] TrainLoss: 0.000641  ValLoss: 0.000159  Val RMSE: 0.0126  Val NMSE: 0.0032  Val NMSE_dB: -24.955658 dB  TrainTime: 1.06s\n",
      "[48/050] TrainLoss: 0.000639  ValLoss: 0.000123  Val RMSE: 0.0111  Val NMSE: 0.0025  Val NMSE_dB: -26.055983 dB  TrainTime: 1.00s\n",
      "[49/050] TrainLoss: 0.000636  ValLoss: 0.000200  Val RMSE: 0.0141  Val NMSE: 0.0040  Val NMSE_dB: -23.952396 dB  TrainTime: 1.06s\n",
      "[50/050] TrainLoss: 0.000647  ValLoss: 0.000183  Val RMSE: 0.0135  Val NMSE: 0.0037  Val NMSE_dB: -24.342134 dB  TrainTime: 0.99s\n",
      "============================================================\n",
      "=> Best ValLoss   : 0.000098   (epoch 25)\n",
      "=> Best Val NMSE  : 0.0020   (epoch 25)\n",
      "=> Best Val RMSE  : 0.0099   (epoch 25)\n",
      "=> Best Val NMSE_dB: -27.0545 dB (epoch 25)\n",
      "============================================================\n",
      "\n",
      "MLP training log saved to /home/dlghdbs200/LWM_denver/MLP.txt\n",
      "\n",
      "=== CNN Training ===\n",
      "=== Training CNN ===\n",
      "[01/050] TrainLoss: 0.041543  ValLoss: 0.027705  Val RMSE: 0.1664  Val NMSE: 0.5576  Val NMSE_dB: -2.536706 dB  TrainTime: 2.04s\n",
      "[02/050] TrainLoss: 0.018881  ValLoss: 0.012398  Val RMSE: 0.1113  Val NMSE: 0.2496  Val NMSE_dB: -6.028239 dB  TrainTime: 1.11s\n",
      "[03/050] TrainLoss: 0.005915  ValLoss: 0.003176  Val RMSE: 0.0564  Val NMSE: 0.0639  Val NMSE_dB: -11.943042 dB  TrainTime: 1.04s\n",
      "[04/050] TrainLoss: 0.001047  ValLoss: 0.000966  Val RMSE: 0.0311  Val NMSE: 0.0194  Val NMSE_dB: -17.112346 dB  TrainTime: 1.24s\n",
      "[05/050] TrainLoss: 0.000498  ValLoss: 0.000395  Val RMSE: 0.0199  Val NMSE: 0.0080  Val NMSE_dB: -20.993580 dB  TrainTime: 1.25s\n",
      "[06/050] TrainLoss: 0.000364  ValLoss: 0.000348  Val RMSE: 0.0186  Val NMSE: 0.0070  Val NMSE_dB: -21.549251 dB  TrainTime: 1.30s\n",
      "[07/050] TrainLoss: 0.000310  ValLoss: 0.000374  Val RMSE: 0.0193  Val NMSE: 0.0075  Val NMSE_dB: -21.229999 dB  TrainTime: 1.30s\n",
      "[08/050] TrainLoss: 0.000283  ValLoss: 0.000381  Val RMSE: 0.0195  Val NMSE: 0.0077  Val NMSE_dB: -21.156257 dB  TrainTime: 0.43s\n",
      "[09/050] TrainLoss: 0.000216  ValLoss: 0.000208  Val RMSE: 0.0144  Val NMSE: 0.0042  Val NMSE_dB: -23.784338 dB  TrainTime: 1.38s\n",
      "[10/050] TrainLoss: 0.000203  ValLoss: 0.000167  Val RMSE: 0.0129  Val NMSE: 0.0034  Val NMSE_dB: -24.744849 dB  TrainTime: 1.25s\n",
      "[11/050] TrainLoss: 0.000184  ValLoss: 0.000125  Val RMSE: 0.0112  Val NMSE: 0.0025  Val NMSE_dB: -25.989055 dB  TrainTime: 1.14s\n",
      "[12/050] TrainLoss: 0.000173  ValLoss: 0.000294  Val RMSE: 0.0171  Val NMSE: 0.0059  Val NMSE_dB: -22.276505 dB  TrainTime: 1.22s\n",
      "[13/050] TrainLoss: 0.000174  ValLoss: 0.000231  Val RMSE: 0.0152  Val NMSE: 0.0046  Val NMSE_dB: -23.333485 dB  TrainTime: 1.22s\n",
      "[14/050] TrainLoss: 0.000164  ValLoss: 0.000138  Val RMSE: 0.0117  Val NMSE: 0.0028  Val NMSE_dB: -25.571464 dB  TrainTime: 1.20s\n",
      "[15/050] TrainLoss: 0.000133  ValLoss: 0.000123  Val RMSE: 0.0111  Val NMSE: 0.0025  Val NMSE_dB: -26.073786 dB  TrainTime: 1.31s\n",
      "[16/050] TrainLoss: 0.000143  ValLoss: 0.000215  Val RMSE: 0.0147  Val NMSE: 0.0043  Val NMSE_dB: -23.629085 dB  TrainTime: 1.21s\n",
      "[17/050] TrainLoss: 0.000149  ValLoss: 0.000135  Val RMSE: 0.0116  Val NMSE: 0.0027  Val NMSE_dB: -25.668688 dB  TrainTime: 1.11s\n",
      "[18/050] TrainLoss: 0.000126  ValLoss: 0.000363  Val RMSE: 0.0191  Val NMSE: 0.0073  Val NMSE_dB: -21.360641 dB  TrainTime: 1.13s\n",
      "[19/050] TrainLoss: 0.000134  ValLoss: 0.000120  Val RMSE: 0.0110  Val NMSE: 0.0024  Val NMSE_dB: -26.167836 dB  TrainTime: 1.31s\n",
      "[20/050] TrainLoss: 0.000116  ValLoss: 0.000066  Val RMSE: 0.0081  Val NMSE: 0.0013  Val NMSE_dB: -28.743907 dB  TrainTime: 1.24s\n",
      "[21/050] TrainLoss: 0.000114  ValLoss: 0.000118  Val RMSE: 0.0109  Val NMSE: 0.0024  Val NMSE_dB: -26.229732 dB  TrainTime: 1.20s\n",
      "[22/050] TrainLoss: 0.000134  ValLoss: 0.000077  Val RMSE: 0.0088  Val NMSE: 0.0016  Val NMSE_dB: -28.080580 dB  TrainTime: 1.24s\n",
      "[23/050] TrainLoss: 0.000081  ValLoss: 0.000194  Val RMSE: 0.0139  Val NMSE: 0.0039  Val NMSE_dB: -24.077613 dB  TrainTime: 1.38s\n",
      "[24/050] TrainLoss: 0.000122  ValLoss: 0.000281  Val RMSE: 0.0168  Val NMSE: 0.0057  Val NMSE_dB: -22.469570 dB  TrainTime: 1.11s\n",
      "[25/050] TrainLoss: 0.000103  ValLoss: 0.000119  Val RMSE: 0.0109  Val NMSE: 0.0024  Val NMSE_dB: -26.193689 dB  TrainTime: 1.12s\n",
      "[26/050] TrainLoss: 0.000087  ValLoss: 0.000113  Val RMSE: 0.0106  Val NMSE: 0.0023  Val NMSE_dB: -26.418820 dB  TrainTime: 1.16s\n",
      "[27/050] TrainLoss: 0.000097  ValLoss: 0.000083  Val RMSE: 0.0091  Val NMSE: 0.0017  Val NMSE_dB: -27.792992 dB  TrainTime: 1.17s\n",
      "[28/050] TrainLoss: 0.000101  ValLoss: 0.000132  Val RMSE: 0.0115  Val NMSE: 0.0027  Val NMSE_dB: -25.739263 dB  TrainTime: 1.22s\n",
      "[29/050] TrainLoss: 0.000087  ValLoss: 0.000274  Val RMSE: 0.0166  Val NMSE: 0.0055  Val NMSE_dB: -22.575694 dB  TrainTime: 1.23s\n",
      "[30/050] TrainLoss: 0.000123  ValLoss: 0.000118  Val RMSE: 0.0109  Val NMSE: 0.0024  Val NMSE_dB: -26.240423 dB  TrainTime: 1.20s\n",
      "[31/050] TrainLoss: 0.000077  ValLoss: 0.000082  Val RMSE: 0.0091  Val NMSE: 0.0017  Val NMSE_dB: -27.821512 dB  TrainTime: 1.29s\n",
      "[32/050] TrainLoss: 0.000097  ValLoss: 0.000042  Val RMSE: 0.0064  Val NMSE: 0.0008  Val NMSE_dB: -30.779673 dB  TrainTime: 1.24s\n",
      "[33/050] TrainLoss: 0.000080  ValLoss: 0.000052  Val RMSE: 0.0072  Val NMSE: 0.0010  Val NMSE_dB: -29.805281 dB  TrainTime: 1.21s\n",
      "[34/050] TrainLoss: 0.000077  ValLoss: 0.000060  Val RMSE: 0.0078  Val NMSE: 0.0012  Val NMSE_dB: -29.152088 dB  TrainTime: 0.42s\n",
      "[35/050] TrainLoss: 0.000087  ValLoss: 0.000133  Val RMSE: 0.0115  Val NMSE: 0.0027  Val NMSE_dB: -25.715235 dB  TrainTime: 1.31s\n",
      "[36/050] TrainLoss: 0.000095  ValLoss: 0.000057  Val RMSE: 0.0075  Val NMSE: 0.0011  Val NMSE_dB: -29.428096 dB  TrainTime: 1.12s\n",
      "[37/050] TrainLoss: 0.000078  ValLoss: 0.000098  Val RMSE: 0.0099  Val NMSE: 0.0020  Val NMSE_dB: -27.055140 dB  TrainTime: 1.13s\n",
      "[38/050] TrainLoss: 0.000072  ValLoss: 0.000115  Val RMSE: 0.0107  Val NMSE: 0.0023  Val NMSE_dB: -26.352723 dB  TrainTime: 1.20s\n",
      "[39/050] TrainLoss: 0.000076  ValLoss: 0.000085  Val RMSE: 0.0092  Val NMSE: 0.0017  Val NMSE_dB: -27.658609 dB  TrainTime: 1.18s\n",
      "[40/050] TrainLoss: 0.000085  ValLoss: 0.000054  Val RMSE: 0.0073  Val NMSE: 0.0011  Val NMSE_dB: -29.645448 dB  TrainTime: 1.29s\n",
      "[41/050] TrainLoss: 0.000075  ValLoss: 0.000144  Val RMSE: 0.0120  Val NMSE: 0.0029  Val NMSE_dB: -25.373188 dB  TrainTime: 1.09s\n",
      "[42/050] TrainLoss: 0.000097  ValLoss: 0.000103  Val RMSE: 0.0102  Val NMSE: 0.0021  Val NMSE_dB: -26.817781 dB  TrainTime: 1.12s\n",
      "[43/050] TrainLoss: 0.000052  ValLoss: 0.000043  Val RMSE: 0.0065  Val NMSE: 0.0009  Val NMSE_dB: -30.646054 dB  TrainTime: 1.15s\n",
      "[44/050] TrainLoss: 0.000064  ValLoss: 0.000105  Val RMSE: 0.0102  Val NMSE: 0.0021  Val NMSE_dB: -26.758115 dB  TrainTime: 1.20s\n",
      "[45/050] TrainLoss: 0.000079  ValLoss: 0.000053  Val RMSE: 0.0073  Val NMSE: 0.0011  Val NMSE_dB: -29.685497 dB  TrainTime: 1.18s\n",
      "[46/050] TrainLoss: 0.000052  ValLoss: 0.000157  Val RMSE: 0.0125  Val NMSE: 0.0032  Val NMSE_dB: -24.993270 dB  TrainTime: 1.21s\n",
      "[47/050] TrainLoss: 0.000080  ValLoss: 0.000416  Val RMSE: 0.0204  Val NMSE: 0.0084  Val NMSE_dB: -20.774160 dB  TrainTime: 1.30s\n",
      "[48/050] TrainLoss: 0.000068  ValLoss: 0.000109  Val RMSE: 0.0104  Val NMSE: 0.0022  Val NMSE_dB: -26.592499 dB  TrainTime: 1.16s\n",
      "[49/050] TrainLoss: 0.000064  ValLoss: 0.000053  Val RMSE: 0.0073  Val NMSE: 0.0011  Val NMSE_dB: -29.697283 dB  TrainTime: 1.20s\n",
      "[50/050] TrainLoss: 0.000065  ValLoss: 0.000116  Val RMSE: 0.0107  Val NMSE: 0.0023  Val NMSE_dB: -26.333407 dB  TrainTime: 1.24s\n",
      "============================================================\n",
      "=> Best ValLoss   : 0.000042   (epoch 32)\n",
      "=> Best Val NMSE  : 0.0008   (epoch 32)\n",
      "=> Best Val RMSE  : 0.0064   (epoch 32)\n",
      "=> Best Val NMSE_dB: -30.7797 dB (epoch 32)\n",
      "============================================================\n",
      "\n",
      "CNN training log saved to /home/dlghdbs200/LWM_denver/CNN.txt\n",
      "\n",
      "=== LWM Training ===\n",
      "=== Training LWM ===\n",
      "[01/050] TrainLoss: 0.088002  ValLoss: 0.049629  Val RMSE: 0.2228  Val NMSE: 0.9989  Val NMSE_dB: -0.004575 dB  TrainTime: 8.54s\n",
      "[02/050] TrainLoss: 0.039368  ValLoss: 0.006120  Val RMSE: 0.0782  Val NMSE: 0.1232  Val NMSE_dB: -9.094412 dB  TrainTime: 7.30s\n",
      "[03/050] TrainLoss: 0.005238  ValLoss: 0.000692  Val RMSE: 0.0263  Val NMSE: 0.0139  Val NMSE_dB: -18.556914 dB  TrainTime: 7.92s\n",
      "[04/050] TrainLoss: 0.002027  ValLoss: 0.000279  Val RMSE: 0.0167  Val NMSE: 0.0056  Val NMSE_dB: -22.501094 dB  TrainTime: 8.16s\n",
      "[05/050] TrainLoss: 0.001402  ValLoss: 0.000176  Val RMSE: 0.0133  Val NMSE: 0.0035  Val NMSE_dB: -24.502935 dB  TrainTime: 8.51s\n",
      "[06/050] TrainLoss: 0.001053  ValLoss: 0.000171  Val RMSE: 0.0131  Val NMSE: 0.0034  Val NMSE_dB: -24.634921 dB  TrainTime: 7.41s\n",
      "[07/050] TrainLoss: 0.000862  ValLoss: 0.000101  Val RMSE: 0.0100  Val NMSE: 0.0020  Val NMSE_dB: -26.917335 dB  TrainTime: 8.39s\n",
      "[08/050] TrainLoss: 0.000744  ValLoss: 0.000133  Val RMSE: 0.0115  Val NMSE: 0.0027  Val NMSE_dB: -25.737356 dB  TrainTime: 8.19s\n",
      "[09/050] TrainLoss: 0.000651  ValLoss: 0.000088  Val RMSE: 0.0094  Val NMSE: 0.0018  Val NMSE_dB: -27.521067 dB  TrainTime: 7.91s\n",
      "[10/050] TrainLoss: 0.000584  ValLoss: 0.000097  Val RMSE: 0.0098  Val NMSE: 0.0020  Val NMSE_dB: -27.092226 dB  TrainTime: 7.31s\n",
      "[11/050] TrainLoss: 0.000530  ValLoss: 0.000056  Val RMSE: 0.0075  Val NMSE: 0.0011  Val NMSE_dB: -29.441334 dB  TrainTime: 8.07s\n",
      "[12/050] TrainLoss: 0.000485  ValLoss: 0.000077  Val RMSE: 0.0088  Val NMSE: 0.0015  Val NMSE_dB: -28.106118 dB  TrainTime: 8.00s\n",
      "[13/050] TrainLoss: 0.000445  ValLoss: 0.000049  Val RMSE: 0.0070  Val NMSE: 0.0010  Val NMSE_dB: -30.033263 dB  TrainTime: 8.56s\n",
      "[14/050] TrainLoss: 0.000410  ValLoss: 0.000069  Val RMSE: 0.0083  Val NMSE: 0.0014  Val NMSE_dB: -28.563760 dB  TrainTime: 8.31s\n",
      "[15/050] TrainLoss: 0.000385  ValLoss: 0.000055  Val RMSE: 0.0074  Val NMSE: 0.0011  Val NMSE_dB: -29.533363 dB  TrainTime: 9.19s\n",
      "[16/050] TrainLoss: 0.000359  ValLoss: 0.000046  Val RMSE: 0.0068  Val NMSE: 0.0009  Val NMSE_dB: -30.372173 dB  TrainTime: 8.22s\n",
      "[17/050] TrainLoss: 0.000335  ValLoss: 0.000051  Val RMSE: 0.0072  Val NMSE: 0.0010  Val NMSE_dB: -29.854629 dB  TrainTime: 8.05s\n",
      "[18/050] TrainLoss: 0.000316  ValLoss: 0.000063  Val RMSE: 0.0079  Val NMSE: 0.0013  Val NMSE_dB: -28.972037 dB  TrainTime: 7.17s\n",
      "[19/050] TrainLoss: 0.000301  ValLoss: 0.000042  Val RMSE: 0.0065  Val NMSE: 0.0008  Val NMSE_dB: -30.706508 dB  TrainTime: 8.05s\n",
      "[20/050] TrainLoss: 0.000282  ValLoss: 0.000027  Val RMSE: 0.0052  Val NMSE: 0.0006  Val NMSE_dB: -32.591495 dB  TrainTime: 7.77s\n",
      "[21/050] TrainLoss: 0.000267  ValLoss: 0.000050  Val RMSE: 0.0071  Val NMSE: 0.0010  Val NMSE_dB: -29.994674 dB  TrainTime: 7.75s\n",
      "[22/050] TrainLoss: 0.000252  ValLoss: 0.000035  Val RMSE: 0.0059  Val NMSE: 0.0007  Val NMSE_dB: -31.536269 dB  TrainTime: 6.94s\n",
      "[23/050] TrainLoss: 0.000242  ValLoss: 0.000040  Val RMSE: 0.0063  Val NMSE: 0.0008  Val NMSE_dB: -30.932277 dB  TrainTime: 7.71s\n",
      "[24/050] TrainLoss: 0.000229  ValLoss: 0.000124  Val RMSE: 0.0112  Val NMSE: 0.0025  Val NMSE_dB: -26.014128 dB  TrainTime: 7.94s\n",
      "[25/050] TrainLoss: 0.000222  ValLoss: 0.000028  Val RMSE: 0.0053  Val NMSE: 0.0006  Val NMSE_dB: -32.482709 dB  TrainTime: 7.63s\n",
      "[26/050] TrainLoss: 0.000210  ValLoss: 0.000061  Val RMSE: 0.0078  Val NMSE: 0.0012  Val NMSE_dB: -29.106674 dB  TrainTime: 6.95s\n",
      "[27/050] TrainLoss: 0.000200  ValLoss: 0.000040  Val RMSE: 0.0063  Val NMSE: 0.0008  Val NMSE_dB: -30.908592 dB  TrainTime: 7.72s\n",
      "[28/050] TrainLoss: 0.000192  ValLoss: 0.000065  Val RMSE: 0.0081  Val NMSE: 0.0013  Val NMSE_dB: -28.838960 dB  TrainTime: 7.77s\n",
      "[29/050] TrainLoss: 0.000188  ValLoss: 0.000062  Val RMSE: 0.0079  Val NMSE: 0.0012  Val NMSE_dB: -29.064268 dB  TrainTime: 7.95s\n",
      "[30/050] TrainLoss: 0.000173  ValLoss: 0.000031  Val RMSE: 0.0056  Val NMSE: 0.0006  Val NMSE_dB: -31.988400 dB  TrainTime: 6.89s\n",
      "[31/050] TrainLoss: 0.000168  ValLoss: 0.000065  Val RMSE: 0.0080  Val NMSE: 0.0013  Val NMSE_dB: -28.863100 dB  TrainTime: 7.73s\n",
      "[32/050] TrainLoss: 0.000162  ValLoss: 0.000119  Val RMSE: 0.0109  Val NMSE: 0.0024  Val NMSE_dB: -26.211496 dB  TrainTime: 7.98s\n",
      "[33/050] TrainLoss: 0.000162  ValLoss: 0.000028  Val RMSE: 0.0053  Val NMSE: 0.0006  Val NMSE_dB: -32.414630 dB  TrainTime: 7.67s\n",
      "[34/050] TrainLoss: 0.000149  ValLoss: 0.000055  Val RMSE: 0.0074  Val NMSE: 0.0011  Val NMSE_dB: -29.527234 dB  TrainTime: 6.96s\n",
      "[35/050] TrainLoss: 0.000144  ValLoss: 0.000013  Val RMSE: 0.0037  Val NMSE: 0.0003  Val NMSE_dB: -35.691519 dB  TrainTime: 8.14s\n",
      "[36/050] TrainLoss: 0.000141  ValLoss: 0.000055  Val RMSE: 0.0074  Val NMSE: 0.0011  Val NMSE_dB: -29.589756 dB  TrainTime: 8.15s\n",
      "[37/050] TrainLoss: 0.000140  ValLoss: 0.000037  Val RMSE: 0.0061  Val NMSE: 0.0007  Val NMSE_dB: -31.285879 dB  TrainTime: 8.19s\n",
      "[38/050] TrainLoss: 0.000127  ValLoss: 0.000016  Val RMSE: 0.0040  Val NMSE: 0.0003  Val NMSE_dB: -34.859229 dB  TrainTime: 7.45s\n",
      "[39/050] TrainLoss: 0.000128  ValLoss: 0.000057  Val RMSE: 0.0076  Val NMSE: 0.0012  Val NMSE_dB: -29.387161 dB  TrainTime: 8.13s\n",
      "[40/050] TrainLoss: 0.000128  ValLoss: 0.000045  Val RMSE: 0.0067  Val NMSE: 0.0009  Val NMSE_dB: -30.408068 dB  TrainTime: 8.05s\n",
      "[41/050] TrainLoss: 0.000122  ValLoss: 0.000105  Val RMSE: 0.0103  Val NMSE: 0.0021  Val NMSE_dB: -26.741342 dB  TrainTime: 7.50s\n",
      "[42/050] TrainLoss: 0.000118  ValLoss: 0.000067  Val RMSE: 0.0082  Val NMSE: 0.0013  Val NMSE_dB: -28.724117 dB  TrainTime: 7.04s\n",
      "[43/050] TrainLoss: 0.000118  ValLoss: 0.000074  Val RMSE: 0.0086  Val NMSE: 0.0015  Val NMSE_dB: -28.250135 dB  TrainTime: 7.82s\n",
      "[44/050] TrainLoss: 0.000113  ValLoss: 0.000033  Val RMSE: 0.0058  Val NMSE: 0.0007  Val NMSE_dB: -31.742042 dB  TrainTime: 7.74s\n",
      "[45/050] TrainLoss: 0.000106  ValLoss: 0.000041  Val RMSE: 0.0064  Val NMSE: 0.0008  Val NMSE_dB: -30.797715 dB  TrainTime: 7.89s\n",
      "[46/050] TrainLoss: 0.000111  ValLoss: 0.000050  Val RMSE: 0.0070  Val NMSE: 0.0010  Val NMSE_dB: -30.004612 dB  TrainTime: 6.85s\n",
      "[47/050] TrainLoss: 0.000105  ValLoss: 0.000028  Val RMSE: 0.0053  Val NMSE: 0.0006  Val NMSE_dB: -32.441868 dB  TrainTime: 7.68s\n",
      "[48/050] TrainLoss: 0.000100  ValLoss: 0.000051  Val RMSE: 0.0071  Val NMSE: 0.0010  Val NMSE_dB: -29.927602 dB  TrainTime: 7.89s\n",
      "[49/050] TrainLoss: 0.000096  ValLoss: 0.000054  Val RMSE: 0.0073  Val NMSE: 0.0011  Val NMSE_dB: -29.655170 dB  TrainTime: 7.66s\n",
      "[50/050] TrainLoss: 0.000099  ValLoss: 0.000039  Val RMSE: 0.0063  Val NMSE: 0.0008  Val NMSE_dB: -31.009988 dB  TrainTime: 7.22s\n",
      "============================================================\n",
      "=> Best ValLoss   : 0.000013   (epoch 35)\n",
      "=> Best Val NMSE  : 0.0003   (epoch 35)\n",
      "=> Best Val RMSE  : 0.0037   (epoch 35)\n",
      "=> Best Val NMSE_dB: -35.6915 dB (epoch 35)\n",
      "============================================================\n",
      "\n",
      "LWM training log saved to /home/dlghdbs200/LWM_denver/LWM.txt\n"
     ]
    }
   ],
   "source": [
    "import sys, os, contextlib\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# --- Tee that mirrors stdout to both console and a file ---\n",
    "class Tee:\n",
    "    def __init__(self, filename, mode=\"w\", encoding=\"utf-8\"):\n",
    "        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "        self.file = open(filename, mode, encoding=encoding)\n",
    "        self.stdout = sys.stdout\n",
    "\n",
    "    def write(self, data):\n",
    "        self.file.write(data)\n",
    "        self.stdout.write(data)\n",
    "\n",
    "    def flush(self):\n",
    "        self.file.flush()\n",
    "        self.stdout.flush()\n",
    "\n",
    "    def close(self):\n",
    "        self.file.close()\n",
    "\n",
    "    # context manager support\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc, tb):\n",
    "        self.close()\n",
    "        # don't suppress exceptions\n",
    "        return False\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# \n",
    "base_dir = \"/home/dlghdbs200/LWM_denver\"\n",
    "\n",
    "print(\"=== MLP Training ===\")\n",
    "mlp = MLP(hidden=256, depth=3, pdrop=0.1)\n",
    "mlp_path = os.path.join(base_dir, \"MLP.txt\")\n",
    "with Tee(mlp_path, \"w\") as tee:\n",
    "    with contextlib.redirect_stdout(tee):\n",
    "        print(\"=== Training MLP ===\")\n",
    "        run(mlp, epochs=50, lr=2e-3)\n",
    "print(f\"\\nMLP training log saved to {mlp_path}\")\n",
    "\n",
    "print(\"\\n=== CNN Training ===\")\n",
    "cnn = CNN1D()\n",
    "cnn_path = os.path.join(base_dir, \"CNN.txt\")\n",
    "with Tee(cnn_path, \"w\") as tee:\n",
    "    with contextlib.redirect_stdout(tee):\n",
    "        print(\"=== Training CNN ===\")\n",
    "        run(cnn, epochs=50, lr=1e-3)\n",
    "print(f\"\\nCNN training log saved to {cnn_path}\")\n",
    "\n",
    "print(\"\\n=== LWM Training ===\")\n",
    "\n",
    "lwm_model = LWM_denver(d_model=64, n_layers=12)\n",
    "lwm_path = os.path.join(base_dir, \"LWM.txt\")\n",
    "with Tee(lwm_path, \"w\") as tee:\n",
    "    with contextlib.redirect_stdout(tee):\n",
    "        print(\"=== Training LWM ===\")\n",
    "        sample_x, _ = next(iter(train_loader))\n",
    "        L = sample_x.shape[2]\n",
    "        D = sample_x.shape[-1]\n",
    "        lwm_model.build_core(L=L, D=D, device=device)\n",
    "        run(lwm_model, epochs=50, lr=1e-3)\n",
    "print(f\"\\nLWM training log saved to {lwm_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b77083-7fa4-4130-b39b-f0c02ab89563",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
