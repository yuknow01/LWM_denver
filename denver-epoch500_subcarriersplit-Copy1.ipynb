{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02ced925-be5b-4bde-be12-01b379b794da",
   "metadata": {},
   "source": [
    "## dataset denver download if you download than pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ea5f9b0-cf35-48ec-aa33-dc418cd832b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: deepmimo in /home/dlghdbs200/.local/lib/python3.10/site-packages (4.0.0b10)\n",
      "Requirement already satisfied: scipy>=1.6.2 in /home/dlghdbs200/.local/lib/python3.10/site-packages (from deepmimo) (1.15.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from deepmimo) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.59.0 in /home/dlghdbs200/.local/lib/python3.10/site-packages (from deepmimo) (4.67.1)\n",
      "Requirement already satisfied: matplotlib>=3.8.2 in /home/dlghdbs200/.local/lib/python3.10/site-packages (from deepmimo) (3.10.3)\n",
      "Requirement already satisfied: numpy<2.3,>=1.19.5 in /home/dlghdbs200/.local/lib/python3.10/site-packages (from deepmimo) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.8.2->deepmimo) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib>=3.8.2->deepmimo) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/dlghdbs200/.local/lib/python3.10/site-packages (from matplotlib>=3.8.2->deepmimo) (0.12.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/dlghdbs200/.local/lib/python3.10/site-packages (from matplotlib>=3.8.2->deepmimo) (1.3.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/dlghdbs200/.local/lib/python3.10/site-packages (from matplotlib>=3.8.2->deepmimo) (1.4.8)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/dlghdbs200/.local/lib/python3.10/site-packages (from matplotlib>=3.8.2->deepmimo) (4.58.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.8.2->deepmimo) (2.9.0.post0)\n",
      "Requirement already satisfied: pillow>=8 in /home/dlghdbs200/.local/lib/python3.10/site-packages (from matplotlib>=3.8.2->deepmimo) (11.3.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->deepmimo) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->deepmimo) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->deepmimo) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->deepmimo) (2020.6.20)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib>=3.8.2->deepmimo) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install deepmimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33e19fd3-e6ec-4007-bb2f-2ce81ac1aa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepmimo as dm\n",
    "\n",
    "# Search for scenarios matching criteria\n",
    "scenarios = dm.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58402af8-16ae-47c0-9ed1-5b61727634e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario \"city_18_denver_28\" already exists in /home/dlghdbs200/LWM_denver/deepmimo_scenarios\n"
     ]
    }
   ],
   "source": [
    "import deepmimo as dm\n",
    "\n",
    "# Download a specific scenario\n",
    "dm.download('city_18_denver_28')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "213ec8a1-64cb-4102-a9b0-b9bbfb79addf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary, plot_summary\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(summary\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__file__\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from .summary import summary, plot_summary\n",
    "print(summary.__file__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7227ee19-0b5a-4452-ab65-23776cb56d83",
   "metadata": {},
   "source": [
    "# library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c343f294-aa27-4eb8-b6bf-44f358940851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import DeepMIMOv3\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import IterableDataset\n",
    "import numpy as np\n",
    "import time, gc\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from lwm_model import lwm\n",
    "from torch.optim import Adam\n",
    "from pathlib import Path\n",
    "import torch, time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c49d9ec5-08f0-4f80-a3d8-5e1700b9f56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbe816b-f8f0-42a4-a1db-26c752132b9b",
   "metadata": {},
   "source": [
    "# dataset load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9af341ca-03d3-423d-b26c-4169f2d34e3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TXRX PAIR: TXset 1 (tx_idx 0) & RXset 0 (rx_idxs 43248)\n",
      "Loading TXRX PAIR: TXset 2 (tx_idx 0) & RXset 0 (rx_idxs 43248)\n",
      "Loading TXRX PAIR: TXset 3 (tx_idx 0) & RXset 0 (rx_idxs 43248)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating channels: 100%|█████████████████████████████████████████████████████| 43248/43248 [00:01<00:00, 27340.98it/s]\n",
      "Generating channels: 100%|█████████████████████████████████████████████████████| 43248/43248 [00:01<00:00, 26099.02it/s]\n",
      "Generating channels: 100%|█████████████████████████████████████████████████████| 43248/43248 [00:01<00:00, 28216.27it/s]\n"
     ]
    }
   ],
   "source": [
    "import deepmimo as dm\n",
    "\n",
    "# Load dataset\n",
    "dataset = dm.load(\"city_18_denver_28\")\n",
    "\n",
    "# Access dataset properties\n",
    "aoa_az = dataset.aoa_az\n",
    "aoa_el = dataset.aoa_el\n",
    "inter_pos = dataset.inter_pos\n",
    "\n",
    "# Compute specific channel information\n",
    "# los = dataset.compute_los()\n",
    "channels = dataset.compute_channels()\n",
    "pl = dataset.compute_pathloss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bce33f9-9022-4773-bca7-ef7628975879",
   "metadata": {},
   "source": [
    "# Datashape\n",
    "deepmimo/generator/dataset.py def compute_channels in here channel data shape\n",
    "\n",
    "            numpy.ndarray: MIMO channel matrix with shape [n_users, n_rx_ant, n_tx_ant, n_subcarriers]\n",
    "                          if freq_domain=True, otherwise [n_users, n_rx_ant, n_tx_ant, n_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63259a57-d427-4106-b77a-70fc52058535",
   "metadata": {},
   "source": [
    "# three TxSet(1,2,3) \n",
    "“For 43,248 users, 1 user antenna, 8 BS antennas, and 1 propagation path.”\n",
    "(users, user antenna, BS antennas, PL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "823b2047-7527-4d61-bef6-102bafb4385d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43248, 1, 8, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channels[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c927c1fa-dad6-4581-ac78-bc2c65b19b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-4.0636469e-10-2.4687419e-10j],\n",
       "         [ 4.0486328e-10+2.4921543e-10j],\n",
       "         [-4.0334483e-10-2.5152960e-10j],\n",
       "         ...,\n",
       "         [ 3.9869291e-10+2.5830790e-10j],\n",
       "         [-3.9711204e-10-2.6051203e-10j],\n",
       "         [ 3.9551701e-10+2.6268823e-10j]]],\n",
       "\n",
       "\n",
       "       [[[ 1.5214810e-10-7.0644335e-10j],\n",
       "         [-1.5652508e-10+7.0226053e-10j],\n",
       "         [ 1.6082487e-10-6.9803024e-10j],\n",
       "         ...,\n",
       "         [-1.7325429e-10+6.8506756e-10j],\n",
       "         [ 1.7723865e-10-6.8066053e-10j],\n",
       "         [-1.8114261e-10+6.7621270e-10j]]],\n",
       "\n",
       "\n",
       "       [[[ 7.6910278e-10+1.6763185e-11j],\n",
       "         [-7.6921619e-10-1.8937532e-11j],\n",
       "         [ 7.6933526e-10+2.1130594e-11j],\n",
       "         ...,\n",
       "         [-7.6972168e-10-2.7824850e-11j],\n",
       "         [ 7.6985857e-10+3.0095502e-11j],\n",
       "         [-7.6999856e-10-3.2386184e-11j]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ 0.0000000e+00+0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j],\n",
       "         ...,\n",
       "         [ 0.0000000e+00+0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j]]],\n",
       "\n",
       "\n",
       "       [[[ 0.0000000e+00+0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j],\n",
       "         ...,\n",
       "         [ 0.0000000e+00+0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j]]],\n",
       "\n",
       "\n",
       "       [[[ 0.0000000e+00+0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j],\n",
       "         ...,\n",
       "         [ 0.0000000e+00+0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j]]]],\n",
       "      shape=(43248, 1, 8, 1), dtype=complex64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e7a0995-f292-46b0-9f9c-1e11162cceb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(channels[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2832596c-936e-402e-b9db-70a75e79c58b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.3398023e-09-1.4365638e-09j], dtype=complex64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channels[1][2][0][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b5b8bf-d6c5-4584-9a92-5e6dbecf8827",
   "metadata": {},
   "source": [
    "# data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94d9cd7-da7a-4695-a61a-5ab7f4f0a58e",
   "metadata": {},
   "source": [
    "## How to predict \n",
    "H1,H2,H3 -> all data union <br>\n",
    "Tx_antenna split <br>\n",
    "Sliding-window version: Predicts the next antenna response from a few adjacent antennas (local spatial reconstruction).\n",
    "\n",
    "In summary, this code converts complex antenna channel data into real-valued, normalized tensors and prepares it for training.\n",
    "\n",
    "Three complex inputs (`H1`, `H2`, `H3`, each shaped `(N, 1, 8, 1)`) are merged into one dataset `(3N, 1, 8, 1)`. The real and imaginary parts are separated, forming `(3N, 1, 8, 1, 2)`. Each sample is normalized using its L2 norm.\n",
    "\n",
    "A sliding window is then applied along the antenna axis:\n",
    "\n",
    "* **Input:** 3 consecutive antennas (`win_in = 3`)\n",
    "* **Target:** the next antenna (`win_out = 1`)\n",
    "\n",
    "This produces 5 windows per sample (`(num_pos * 3N, 1, 3, 1, 2)` for inputs and `(num_pos * 3N, 1, 1, 1, 2)` for targets).\n",
    "\n",
    "Finally, data is split by antenna index:\n",
    "\n",
    "* **Training:** target index < 6\n",
    "* **Validation:** target index ≥ 6\n",
    "\n",
    "Both subsets are converted into PyTorch `DataLoader`s for efficient training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22876e68-5361-4158-a252-df1ec766095d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windows per sample: 5\n",
      "Shapes -> X_win: torch.Size([648720, 1, 3, 1, 2]) Y_win: torch.Size([648720, 1, 1, 1, 2])\n",
      "Train/Val samples: 389232 / 259488\n",
      "Example shapes -> X: torch.Size([389232, 1, 3, 1, 2]), y: torch.Size([389232, 1, 1, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch, numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# ---------- 0) Load & combine three channels ----------\n",
    "H1, H2, H3 = channels[0], channels[1], channels[2]     # each: (N, 1, 8, 1), complex\n",
    "X = np.concatenate([H1, H2, H3], axis=0)               # (3N, 1, 8, 1)\n",
    "y = X.copy()\n",
    "\n",
    "# ---------- 1) Real/Imag split ----------\n",
    "X = np.stack([X.real, X.imag], axis=-1).astype(np.float32)  # (3N, 1, 8, 1, 2)\n",
    "y = np.stack([y.real, y.imag], axis=-1).astype(np.float32)\n",
    "\n",
    "# ---------- 2) Torch + per-sample normalization ----------\n",
    "X_t = torch.from_numpy(X).float()\n",
    "y_t = torch.from_numpy(y).float()\n",
    "eps = 1e-8\n",
    "scale = torch.linalg.vector_norm(X_t.view(X_t.size(0), -1), dim=1, keepdim=True).clamp_min(eps)\n",
    "X_t = X_t / scale.view(-1,1,1,1,1)\n",
    "y_t = y_t / scale.view(-1,1,1,1,1)\n",
    "\n",
    "# ---------- 3) Build antenna-axis sliding windows (3 in -> 1 out) ----------\n",
    "N3, _, n_tx, _, _ = X_t.shape     # n_tx should be 8 here\n",
    "assert n_tx >= 4, \"Need at least 4 antennas for 3->1 split.\"\n",
    "win_in, win_out = 3, 1\n",
    "num_pos = n_tx - (win_in + win_out) + 1      # 8 - 4 + 1 = 5\n",
    "\n",
    "X_chunks, Y_chunks, tgt_idx_list = [], [], []\n",
    "for p in range(num_pos):\n",
    "    X_chunks.append(X_t[:, :, p:p+win_in, :, :])                  # (3N,1,3,1,2)\n",
    "    Y_chunks.append(y_t[:, :, p+win_in:p+win_in+win_out, :, :])   # (3N,1,1,1,2)\n",
    "    tgt_idx_list.append(torch.full((N3,), p+win_in, dtype=torch.long))\n",
    "\n",
    "X_win = torch.cat(X_chunks, dim=0)     # (num_pos*3N, 1, 3, 1, 2)\n",
    "Y_win = torch.cat(Y_chunks, dim=0)     # (num_pos*3N, 1, 1, 1, 2)\n",
    "tgt_idx = torch.cat(tgt_idx_list, dim=0)  # (num_pos*3N,)\n",
    "print(\"Windows per sample:\", num_pos)\n",
    "print(\"Shapes -> X_win:\", X_win.shape, \"Y_win:\", Y_win.shape)\n",
    "\n",
    "# ---------- 4) Split by target-antenna: train(0..5), val(6..7) ----------\n",
    "train_ant = 6                              # target idx < 6 => train\n",
    "mask_tr = (tgt_idx < train_ant)\n",
    "mask_va = ~mask_tr\n",
    "\n",
    "X_tr, Y_tr = X_win[mask_tr], Y_win[mask_tr]\n",
    "X_va, Y_va = X_win[mask_va], Y_win[mask_va]\n",
    "\n",
    "# ---------- 5) DataLoaders (dataset carries only X,Y to avoid unpack errors) ----------\n",
    "train_set = TensorDataset(X_tr, Y_tr)\n",
    "val_set   = TensorDataset(X_va, Y_va)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=1024, shuffle=True)\n",
    "val_loader   = DataLoader(val_set,   batch_size=2048, shuffle=False)\n",
    "\n",
    "print(f\"Train/Val samples: {len(train_set)} / {len(val_set)}\")\n",
    "print(f\"Example shapes -> X: {X_tr.shape}, y: {Y_tr.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72097380-6890-4db8-9d76-c99bddb28fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129744, 1, 8, 1, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9729836e-c0b0-4a03-8483-1a59eb33432d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129744, 1, 8, 1, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48fc006b-a516-434d-967a-5f842642139f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0., 0.]],\n",
       " \n",
       "          [[0., 0.]],\n",
       " \n",
       "          [[0., 0.]]]]),\n",
       " tensor([[[[0., 0.]]]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_set[12000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df1190e-f2d5-425a-85eb-e23ffaaa6d5f",
   "metadata": {},
   "source": [
    "# Model define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecce1ee9-1398-4b28-b02c-38cc74a1df2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden=256, depth=3, pdrop=0.1):\n",
    "        super().__init__()\n",
    "        d_in, d_out = 3*2, 1*2  # (3 antennas * real+imag, 1 antenna * real+imag)\n",
    "        layers, d = [], d_in\n",
    "        for _ in range(depth - 1):\n",
    "            layers += [nn.Linear(d, hidden), nn.ReLU(), nn.Dropout(pdrop)]\n",
    "            d = hidden\n",
    "        layers += [nn.Linear(d, d_out)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b = x.size(0)\n",
    "        x = x.view(b, -1)        # (B, 6)\n",
    "        y = self.net(x)          # (B, 2)\n",
    "        return y.view(b, 1, 1, 1, 2)\n",
    "\n",
    "\n",
    "\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(2, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv1d(32, 64, 3, padding=1), nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv1d(64, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv1d(32, 2, 3, padding=1)\n",
    "        )\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)  # (B,2,3) -> (B,2,1)\n",
    "\n",
    "    def forward(self, x):                    # x: (B,1,3,1,2)\n",
    "        B, _, L, _, C = x.shape\n",
    "        assert L == 3 and C == 2, f\"expected (B,1,3,1,2), got {x.shape}\"\n",
    "        # (B,1,3,1,2) -> (B,3,1,2) -> (B,2,3)\n",
    "        x = x.squeeze(1).permute(0, 3, 1, 2).squeeze(-1)   # (B,2,3)\n",
    "        z = self.encoder(x)                                # (B,64,3)\n",
    "        z = self.decoder(z)                                # (B,2,3)\n",
    "        z = self.pool(z)                                   # (B,2,1)\n",
    "        return z.permute(0, 2, 1).unsqueeze(1).unsqueeze(3)  # (B,1,1,1,2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4ae9def-0c2d-4758-84f7-2f209042dd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LWM_denver(nn.Module):\n",
    "    def __init__(self, d_model=64, n_layers=12, max_len=129):\n",
    "        super().__init__()\n",
    "        self.core = None\n",
    "        self.config = {\n",
    "            \"d_model\": d_model,\n",
    "            \"n_layers\": n_layers,\n",
    "            \"max_len\": max_len\n",
    "        }\n",
    "\n",
    "    def build_core(self, L, D, device):\n",
    "        from lwm_model import lwm\n",
    "        self.core = lwm(\n",
    "            element_length=D,\n",
    "            d_model=self.config[\"d_model\"],\n",
    "            n_layers=self.config[\"n_layers\"],\n",
    "            max_len=self.config[\"max_len\"]\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, _, L, _, D = x.shape   # (B,1,3,1,2)\n",
    "        x_seq = x.squeeze(1).squeeze(-2).float()  # (B,3,2)\n",
    "        if self.core is None:\n",
    "            self.build_core(L, D, x.device)\n",
    "        # forward\n",
    "        y_hat, _ = self.core(x_seq, torch.arange(L, device=x.device).unsqueeze(0).repeat(B, 1))\n",
    "        # Take last token output only (predict next antenna)\n",
    "        y_last = y_hat[:, -1:, :]  # (B,1,2)\n",
    "        return y_last.unsqueeze(1).unsqueeze(-2)  # (B,1,1,1,2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87433429-9477-4b8f-a1dd-35619b586409",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9229e792-95b4-48a0-b1de-1a579632ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x)\n",
    "        loss = criterion(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, total_mse, total_nmse = 0.0, 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y)\n",
    "            mse = torch.mean((pred - y) ** 2).item()\n",
    "            nmse = mse / torch.mean(y ** 2).item()\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            total_mse += mse * x.size(0)\n",
    "            total_nmse += nmse * x.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    avg_rmse = (total_mse / len(dataloader.dataset)) ** 0.5\n",
    "    avg_nmse = total_nmse / len(dataloader.dataset)\n",
    "    avg_nmse_db = 10 * np.log10(avg_nmse + 1e-12)\n",
    "    return avg_loss, avg_rmse, avg_nmse, avg_nmse_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "991e24de-b922-4932-a2b4-4b541cf7e750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model, epochs=150, lr=1e-3, weight_decay=0.0, show_every=1):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss(reduction=\"mean\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    best_nmse = float(\"inf\")\n",
    "    best_rmse = float(\"inf\")\n",
    "    best_nmse_db = float(\"inf\")\n",
    "\n",
    "    best_epoch_loss = 0\n",
    "    best_epoch_nmse = 0\n",
    "    best_epoch_rmse = 0\n",
    "    best_epoch_nmse_db = 0\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        t0 = time.time()\n",
    "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_loss, rmse, nmse, nmse_db = evaluate(model, val_loader, criterion)\n",
    "        dt = time.time() - t0\n",
    "\n",
    "        # Track best values\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_epoch_loss = ep\n",
    "\n",
    "        if nmse < best_nmse:\n",
    "            best_nmse = nmse\n",
    "            best_epoch_nmse = ep\n",
    "\n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_epoch_rmse = ep\n",
    "\n",
    "        if nmse_db < best_nmse_db:\n",
    "            best_nmse_db = nmse_db\n",
    "            best_epoch_nmse_db = ep\n",
    "\n",
    "        # Logging\n",
    "        if ep % show_every == 0:\n",
    "            print(f\"[{ep:02d}/{epochs:03d}] \"\n",
    "                  f\"TrainLoss: {train_loss:.6f}  \"\n",
    "                  f\"ValLoss: {val_loss:.6f}  \"\n",
    "                  f\"Val RMSE: {rmse:.4f}  \"\n",
    "                  f\"Val NMSE: {nmse:.4f}  \"\n",
    "                  f\"Val NMSE_dB: {nmse_db:.6f} dB  \"\n",
    "                  f\"TrainTime: {dt:.2f}s\")\n",
    "\n",
    "    # Summary of best metrics\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"=> Best ValLoss   : {best_loss:.6f}   (epoch {best_epoch_loss})\")\n",
    "    print(f\"=> Best Val NMSE  : {best_nmse:.4f}   (epoch {best_epoch_nmse})\")\n",
    "    print(f\"=> Best Val RMSE  : {best_rmse:.4f}   (epoch {best_epoch_rmse})\")\n",
    "    print(f\"=> Best Val NMSE_dB: {best_nmse_db:.4f} dB (epoch {best_epoch_nmse_db})\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95d40ca9-e4ef-4397-9d25-9730d902a42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MLP Training ===\n",
      "=== Training MLP ===\n",
      "[01/001] TrainLoss: 0.010099  ValLoss: 0.006801  Val RMSE: 0.0825  Val NMSE: 0.2371  Val NMSE_dB: -6.249932 dB  TrainTime: 5.08s\n",
      "============================================================\n",
      "=> Best ValLoss   : 0.006801   (epoch 1)\n",
      "=> Best Val NMSE  : 0.2371   (epoch 1)\n",
      "=> Best Val RMSE  : 0.0825   (epoch 1)\n",
      "=> Best Val NMSE_dB: -6.2499 dB (epoch 1)\n",
      "============================================================\n",
      "\n",
      "MLP training log saved to /home/dlghdbs200/LWM_denver/MLP_epoch5_antenna.txt\n",
      "\n",
      "=== CNN Training ===\n",
      "=== Training CNN ===\n",
      "[01/001] TrainLoss: 0.012940  ValLoss: 0.007175  Val RMSE: 0.0847  Val NMSE: 0.2506  Val NMSE_dB: -6.009627 dB  TrainTime: 5.92s\n",
      "============================================================\n",
      "=> Best ValLoss   : 0.007175   (epoch 1)\n",
      "=> Best Val NMSE  : 0.2506   (epoch 1)\n",
      "=> Best Val RMSE  : 0.0847   (epoch 1)\n",
      "=> Best Val NMSE_dB: -6.0096 dB (epoch 1)\n",
      "============================================================\n",
      "\n",
      "CNN training log saved to /home/dlghdbs200/LWM_denver/CNN_epoch5_antenna.txt\n",
      "\n",
      "=== LWM Training ===\n",
      "=== Training LWM ===\n",
      "[01/150] TrainLoss: 0.034498  ValLoss: 0.026478  Val RMSE: 0.1627  Val NMSE: 0.9318  Val NMSE_dB: -0.306701 dB  TrainTime: 27.46s\n",
      "[02/150] TrainLoss: 0.016886  ValLoss: 0.008052  Val RMSE: 0.0897  Val NMSE: 0.2817  Val NMSE_dB: -5.501832 dB  TrainTime: 28.35s\n",
      "[03/150] TrainLoss: 0.008144  ValLoss: 0.006980  Val RMSE: 0.0835  Val NMSE: 0.2448  Val NMSE_dB: -6.112752 dB  TrainTime: 29.95s\n",
      "[04/150] TrainLoss: 0.007370  ValLoss: 0.006767  Val RMSE: 0.0823  Val NMSE: 0.2378  Val NMSE_dB: -6.238443 dB  TrainTime: 30.17s\n",
      "[05/150] TrainLoss: 0.006983  ValLoss: 0.006531  Val RMSE: 0.0808  Val NMSE: 0.2299  Val NMSE_dB: -6.385355 dB  TrainTime: 31.51s\n",
      "[06/150] TrainLoss: 0.006737  ValLoss: 0.006698  Val RMSE: 0.0818  Val NMSE: 0.2373  Val NMSE_dB: -6.246545 dB  TrainTime: 30.54s\n",
      "[07/150] TrainLoss: 0.006582  ValLoss: 0.006413  Val RMSE: 0.0801  Val NMSE: 0.2255  Val NMSE_dB: -6.468886 dB  TrainTime: 29.80s\n",
      "[08/150] TrainLoss: 0.006421  ValLoss: 0.006370  Val RMSE: 0.0798  Val NMSE: 0.2248  Val NMSE_dB: -6.481504 dB  TrainTime: 30.20s\n",
      "[09/150] TrainLoss: 0.006346  ValLoss: 0.006378  Val RMSE: 0.0799  Val NMSE: 0.2250  Val NMSE_dB: -6.477755 dB  TrainTime: 31.09s\n",
      "[10/150] TrainLoss: 0.006283  ValLoss: 0.006170  Val RMSE: 0.0785  Val NMSE: 0.2171  Val NMSE_dB: -6.632496 dB  TrainTime: 28.19s\n",
      "[11/150] TrainLoss: 0.006190  ValLoss: 0.006141  Val RMSE: 0.0784  Val NMSE: 0.2167  Val NMSE_dB: -6.641354 dB  TrainTime: 33.36s\n",
      "[12/150] TrainLoss: 0.006130  ValLoss: 0.006146  Val RMSE: 0.0784  Val NMSE: 0.2167  Val NMSE_dB: -6.641350 dB  TrainTime: 37.68s\n",
      "[13/150] TrainLoss: 0.006061  ValLoss: 0.006316  Val RMSE: 0.0795  Val NMSE: 0.2242  Val NMSE_dB: -6.494049 dB  TrainTime: 32.09s\n",
      "[14/150] TrainLoss: 0.006007  ValLoss: 0.005989  Val RMSE: 0.0774  Val NMSE: 0.2105  Val NMSE_dB: -6.766574 dB  TrainTime: 30.85s\n",
      "[15/150] TrainLoss: 0.005948  ValLoss: 0.006282  Val RMSE: 0.0793  Val NMSE: 0.2216  Val NMSE_dB: -6.544577 dB  TrainTime: 30.81s\n",
      "[16/150] TrainLoss: 0.005939  ValLoss: 0.005938  Val RMSE: 0.0771  Val NMSE: 0.2085  Val NMSE_dB: -6.808424 dB  TrainTime: 30.74s\n",
      "[17/150] TrainLoss: 0.005879  ValLoss: 0.006029  Val RMSE: 0.0776  Val NMSE: 0.2126  Val NMSE_dB: -6.723458 dB  TrainTime: 29.43s\n",
      "[18/150] TrainLoss: 0.005849  ValLoss: 0.005994  Val RMSE: 0.0774  Val NMSE: 0.2107  Val NMSE_dB: -6.763256 dB  TrainTime: 30.92s\n",
      "[19/150] TrainLoss: 0.005833  ValLoss: 0.006032  Val RMSE: 0.0777  Val NMSE: 0.2120  Val NMSE_dB: -6.737258 dB  TrainTime: 28.94s\n",
      "[20/150] TrainLoss: 0.005778  ValLoss: 0.005963  Val RMSE: 0.0772  Val NMSE: 0.2098  Val NMSE_dB: -6.781632 dB  TrainTime: 28.24s\n",
      "[21/150] TrainLoss: 0.005758  ValLoss: 0.006116  Val RMSE: 0.0782  Val NMSE: 0.2151  Val NMSE_dB: -6.674385 dB  TrainTime: 28.82s\n",
      "[22/150] TrainLoss: 0.005743  ValLoss: 0.005924  Val RMSE: 0.0770  Val NMSE: 0.2080  Val NMSE_dB: -6.818418 dB  TrainTime: 29.85s\n",
      "[23/150] TrainLoss: 0.005722  ValLoss: 0.005925  Val RMSE: 0.0770  Val NMSE: 0.2081  Val NMSE_dB: -6.818093 dB  TrainTime: 32.11s\n",
      "[24/150] TrainLoss: 0.005693  ValLoss: 0.005948  Val RMSE: 0.0771  Val NMSE: 0.2094  Val NMSE_dB: -6.789198 dB  TrainTime: 32.63s\n",
      "[25/150] TrainLoss: 0.005658  ValLoss: 0.005895  Val RMSE: 0.0768  Val NMSE: 0.2071  Val NMSE_dB: -6.837580 dB  TrainTime: 35.18s\n",
      "[26/150] TrainLoss: 0.005631  ValLoss: 0.005913  Val RMSE: 0.0769  Val NMSE: 0.2077  Val NMSE_dB: -6.825981 dB  TrainTime: 30.48s\n",
      "[27/150] TrainLoss: 0.005637  ValLoss: 0.005871  Val RMSE: 0.0766  Val NMSE: 0.2060  Val NMSE_dB: -6.861296 dB  TrainTime: 28.21s\n",
      "[28/150] TrainLoss: 0.005616  ValLoss: 0.005969  Val RMSE: 0.0773  Val NMSE: 0.2098  Val NMSE_dB: -6.781526 dB  TrainTime: 28.54s\n",
      "[29/150] TrainLoss: 0.005584  ValLoss: 0.005907  Val RMSE: 0.0769  Val NMSE: 0.2075  Val NMSE_dB: -6.829306 dB  TrainTime: 28.38s\n",
      "[30/150] TrainLoss: 0.005541  ValLoss: 0.006004  Val RMSE: 0.0775  Val NMSE: 0.2113  Val NMSE_dB: -6.750572 dB  TrainTime: 28.43s\n",
      "[31/150] TrainLoss: 0.005560  ValLoss: 0.005985  Val RMSE: 0.0774  Val NMSE: 0.2103  Val NMSE_dB: -6.771099 dB  TrainTime: 28.70s\n",
      "[32/150] TrainLoss: 0.005522  ValLoss: 0.005957  Val RMSE: 0.0772  Val NMSE: 0.2094  Val NMSE_dB: -6.790168 dB  TrainTime: 28.33s\n",
      "[33/150] TrainLoss: 0.005509  ValLoss: 0.005836  Val RMSE: 0.0764  Val NMSE: 0.2045  Val NMSE_dB: -6.894106 dB  TrainTime: 28.73s\n",
      "[34/150] TrainLoss: 0.005479  ValLoss: 0.005865  Val RMSE: 0.0766  Val NMSE: 0.2062  Val NMSE_dB: -6.856356 dB  TrainTime: 28.56s\n",
      "[35/150] TrainLoss: 0.005490  ValLoss: 0.005956  Val RMSE: 0.0772  Val NMSE: 0.2098  Val NMSE_dB: -6.782284 dB  TrainTime: 28.61s\n",
      "[36/150] TrainLoss: 0.005452  ValLoss: 0.005811  Val RMSE: 0.0762  Val NMSE: 0.2036  Val NMSE_dB: -6.911170 dB  TrainTime: 28.19s\n",
      "[37/150] TrainLoss: 0.005432  ValLoss: 0.005866  Val RMSE: 0.0766  Val NMSE: 0.2059  Val NMSE_dB: -6.863304 dB  TrainTime: 28.65s\n",
      "[38/150] TrainLoss: 0.005401  ValLoss: 0.005794  Val RMSE: 0.0761  Val NMSE: 0.2031  Val NMSE_dB: -6.922809 dB  TrainTime: 30.39s\n",
      "[39/150] TrainLoss: 0.005397  ValLoss: 0.005854  Val RMSE: 0.0765  Val NMSE: 0.2055  Val NMSE_dB: -6.871452 dB  TrainTime: 30.52s\n",
      "[40/150] TrainLoss: 0.005383  ValLoss: 0.005864  Val RMSE: 0.0766  Val NMSE: 0.2056  Val NMSE_dB: -6.869186 dB  TrainTime: 30.50s\n",
      "[41/150] TrainLoss: 0.005381  ValLoss: 0.005895  Val RMSE: 0.0768  Val NMSE: 0.2072  Val NMSE_dB: -6.835697 dB  TrainTime: 30.78s\n",
      "[42/150] TrainLoss: 0.005343  ValLoss: 0.005934  Val RMSE: 0.0770  Val NMSE: 0.2087  Val NMSE_dB: -6.803764 dB  TrainTime: 30.46s\n",
      "[43/150] TrainLoss: 0.005335  ValLoss: 0.005820  Val RMSE: 0.0763  Val NMSE: 0.2042  Val NMSE_dB: -6.899904 dB  TrainTime: 28.20s\n",
      "[44/150] TrainLoss: 0.005336  ValLoss: 0.005807  Val RMSE: 0.0762  Val NMSE: 0.2041  Val NMSE_dB: -6.901396 dB  TrainTime: 29.06s\n",
      "[45/150] TrainLoss: 0.005300  ValLoss: 0.006025  Val RMSE: 0.0776  Val NMSE: 0.2124  Val NMSE_dB: -6.729298 dB  TrainTime: 28.38s\n",
      "[46/150] TrainLoss: 0.005297  ValLoss: 0.006066  Val RMSE: 0.0779  Val NMSE: 0.2134  Val NMSE_dB: -6.708841 dB  TrainTime: 28.09s\n",
      "[47/150] TrainLoss: 0.005284  ValLoss: 0.005804  Val RMSE: 0.0762  Val NMSE: 0.2036  Val NMSE_dB: -6.912283 dB  TrainTime: 28.49s\n",
      "[48/150] TrainLoss: 0.005258  ValLoss: 0.005815  Val RMSE: 0.0763  Val NMSE: 0.2043  Val NMSE_dB: -6.896696 dB  TrainTime: 30.13s\n",
      "[49/150] TrainLoss: 0.005249  ValLoss: 0.005876  Val RMSE: 0.0767  Val NMSE: 0.2062  Val NMSE_dB: -6.856387 dB  TrainTime: 28.96s\n",
      "[50/150] TrainLoss: 0.005231  ValLoss: 0.005830  Val RMSE: 0.0764  Val NMSE: 0.2040  Val NMSE_dB: -6.903767 dB  TrainTime: 30.61s\n",
      "[51/150] TrainLoss: 0.005229  ValLoss: 0.006056  Val RMSE: 0.0778  Val NMSE: 0.2137  Val NMSE_dB: -6.701281 dB  TrainTime: 29.51s\n",
      "[52/150] TrainLoss: 0.005233  ValLoss: 0.005885  Val RMSE: 0.0767  Val NMSE: 0.2066  Val NMSE_dB: -6.848001 dB  TrainTime: 28.83s\n",
      "[53/150] TrainLoss: 0.005204  ValLoss: 0.005816  Val RMSE: 0.0763  Val NMSE: 0.2038  Val NMSE_dB: -6.908238 dB  TrainTime: 28.29s\n",
      "[54/150] TrainLoss: 0.005178  ValLoss: 0.005842  Val RMSE: 0.0764  Val NMSE: 0.2047  Val NMSE_dB: -6.889807 dB  TrainTime: 30.15s\n",
      "[55/150] TrainLoss: 0.005205  ValLoss: 0.005853  Val RMSE: 0.0765  Val NMSE: 0.2049  Val NMSE_dB: -6.883893 dB  TrainTime: 31.06s\n",
      "[56/150] TrainLoss: 0.005178  ValLoss: 0.005855  Val RMSE: 0.0765  Val NMSE: 0.2059  Val NMSE_dB: -6.863928 dB  TrainTime: 30.75s\n",
      "[57/150] TrainLoss: 0.005156  ValLoss: 0.005916  Val RMSE: 0.0769  Val NMSE: 0.2079  Val NMSE_dB: -6.822264 dB  TrainTime: 31.15s\n",
      "[58/150] TrainLoss: 0.005171  ValLoss: 0.005870  Val RMSE: 0.0766  Val NMSE: 0.2055  Val NMSE_dB: -6.871294 dB  TrainTime: 29.10s\n",
      "[59/150] TrainLoss: 0.005154  ValLoss: 0.005865  Val RMSE: 0.0766  Val NMSE: 0.2051  Val NMSE_dB: -6.879748 dB  TrainTime: 28.74s\n",
      "[60/150] TrainLoss: 0.005149  ValLoss: 0.005831  Val RMSE: 0.0764  Val NMSE: 0.2046  Val NMSE_dB: -6.891634 dB  TrainTime: 30.41s\n",
      "[61/150] TrainLoss: 0.005120  ValLoss: 0.005874  Val RMSE: 0.0766  Val NMSE: 0.2058  Val NMSE_dB: -6.865058 dB  TrainTime: 28.63s\n",
      "[62/150] TrainLoss: 0.005121  ValLoss: 0.005853  Val RMSE: 0.0765  Val NMSE: 0.2052  Val NMSE_dB: -6.878220 dB  TrainTime: 29.92s\n",
      "[63/150] TrainLoss: 0.005105  ValLoss: 0.005848  Val RMSE: 0.0765  Val NMSE: 0.2052  Val NMSE_dB: -6.879019 dB  TrainTime: 29.89s\n",
      "[64/150] TrainLoss: 0.005106  ValLoss: 0.005832  Val RMSE: 0.0764  Val NMSE: 0.2043  Val NMSE_dB: -6.898281 dB  TrainTime: 28.64s\n",
      "[65/150] TrainLoss: 0.005088  ValLoss: 0.005851  Val RMSE: 0.0765  Val NMSE: 0.2055  Val NMSE_dB: -6.872623 dB  TrainTime: 28.46s\n",
      "[66/150] TrainLoss: 0.005074  ValLoss: 0.005791  Val RMSE: 0.0761  Val NMSE: 0.2027  Val NMSE_dB: -6.931629 dB  TrainTime: 28.66s\n",
      "[67/150] TrainLoss: 0.005086  ValLoss: 0.005836  Val RMSE: 0.0764  Val NMSE: 0.2044  Val NMSE_dB: -6.895272 dB  TrainTime: 29.30s\n",
      "[68/150] TrainLoss: 0.005054  ValLoss: 0.005919  Val RMSE: 0.0769  Val NMSE: 0.2071  Val NMSE_dB: -6.837307 dB  TrainTime: 28.90s\n",
      "[69/150] TrainLoss: 0.005052  ValLoss: 0.005870  Val RMSE: 0.0766  Val NMSE: 0.2051  Val NMSE_dB: -6.880545 dB  TrainTime: 28.74s\n",
      "[70/150] TrainLoss: 0.005046  ValLoss: 0.005905  Val RMSE: 0.0768  Val NMSE: 0.2074  Val NMSE_dB: -6.831908 dB  TrainTime: 32.06s\n",
      "[71/150] TrainLoss: 0.005042  ValLoss: 0.005873  Val RMSE: 0.0766  Val NMSE: 0.2058  Val NMSE_dB: -6.864539 dB  TrainTime: 32.18s\n",
      "[72/150] TrainLoss: 0.005025  ValLoss: 0.005977  Val RMSE: 0.0773  Val NMSE: 0.2101  Val NMSE_dB: -6.775463 dB  TrainTime: 30.11s\n",
      "[73/150] TrainLoss: 0.005024  ValLoss: 0.005828  Val RMSE: 0.0763  Val NMSE: 0.2036  Val NMSE_dB: -6.911770 dB  TrainTime: 29.58s\n",
      "[74/150] TrainLoss: 0.005034  ValLoss: 0.006038  Val RMSE: 0.0777  Val NMSE: 0.2124  Val NMSE_dB: -6.728865 dB  TrainTime: 30.16s\n",
      "[75/150] TrainLoss: 0.005059  ValLoss: 0.005854  Val RMSE: 0.0765  Val NMSE: 0.2051  Val NMSE_dB: -6.881214 dB  TrainTime: 30.99s\n",
      "[76/150] TrainLoss: 0.005000  ValLoss: 0.005938  Val RMSE: 0.0771  Val NMSE: 0.2087  Val NMSE_dB: -6.805386 dB  TrainTime: 31.06s\n",
      "[77/150] TrainLoss: 0.004998  ValLoss: 0.005871  Val RMSE: 0.0766  Val NMSE: 0.2057  Val NMSE_dB: -6.868384 dB  TrainTime: 29.33s\n",
      "[78/150] TrainLoss: 0.005014  ValLoss: 0.005878  Val RMSE: 0.0767  Val NMSE: 0.2062  Val NMSE_dB: -6.856764 dB  TrainTime: 28.46s\n",
      "[79/150] TrainLoss: 0.004962  ValLoss: 0.005854  Val RMSE: 0.0765  Val NMSE: 0.2051  Val NMSE_dB: -6.880072 dB  TrainTime: 28.65s\n",
      "[80/150] TrainLoss: 0.004981  ValLoss: 0.005880  Val RMSE: 0.0767  Val NMSE: 0.2055  Val NMSE_dB: -6.872685 dB  TrainTime: 28.42s\n",
      "[81/150] TrainLoss: 0.004954  ValLoss: 0.005925  Val RMSE: 0.0770  Val NMSE: 0.2073  Val NMSE_dB: -6.833167 dB  TrainTime: 28.92s\n",
      "[82/150] TrainLoss: 0.004975  ValLoss: 0.005900  Val RMSE: 0.0768  Val NMSE: 0.2067  Val NMSE_dB: -6.845642 dB  TrainTime: 29.50s\n",
      "[83/150] TrainLoss: 0.004956  ValLoss: 0.005893  Val RMSE: 0.0768  Val NMSE: 0.2066  Val NMSE_dB: -6.848693 dB  TrainTime: 28.99s\n",
      "[84/150] TrainLoss: 0.004967  ValLoss: 0.005929  Val RMSE: 0.0770  Val NMSE: 0.2070  Val NMSE_dB: -6.839620 dB  TrainTime: 29.21s\n",
      "[85/150] TrainLoss: 0.004954  ValLoss: 0.005850  Val RMSE: 0.0765  Val NMSE: 0.2052  Val NMSE_dB: -6.878349 dB  TrainTime: 28.62s\n",
      "[86/150] TrainLoss: 0.004942  ValLoss: 0.005979  Val RMSE: 0.0773  Val NMSE: 0.2097  Val NMSE_dB: -6.783412 dB  TrainTime: 30.58s\n",
      "[87/150] TrainLoss: 0.004958  ValLoss: 0.005953  Val RMSE: 0.0772  Val NMSE: 0.2087  Val NMSE_dB: -6.804938 dB  TrainTime: 29.32s\n",
      "[88/150] TrainLoss: 0.004897  ValLoss: 0.005900  Val RMSE: 0.0768  Val NMSE: 0.2069  Val NMSE_dB: -6.843048 dB  TrainTime: 30.29s\n",
      "[89/150] TrainLoss: 0.004929  ValLoss: 0.005882  Val RMSE: 0.0767  Val NMSE: 0.2060  Val NMSE_dB: -6.862061 dB  TrainTime: 29.84s\n",
      "[90/150] TrainLoss: 0.004914  ValLoss: 0.005862  Val RMSE: 0.0766  Val NMSE: 0.2051  Val NMSE_dB: -6.879769 dB  TrainTime: 30.16s\n",
      "[91/150] TrainLoss: 0.004922  ValLoss: 0.005885  Val RMSE: 0.0767  Val NMSE: 0.2059  Val NMSE_dB: -6.863804 dB  TrainTime: 31.23s\n",
      "[92/150] TrainLoss: 0.004925  ValLoss: 0.006045  Val RMSE: 0.0777  Val NMSE: 0.2124  Val NMSE_dB: -6.728475 dB  TrainTime: 33.16s\n",
      "[93/150] TrainLoss: 0.004926  ValLoss: 0.005903  Val RMSE: 0.0768  Val NMSE: 0.2067  Val NMSE_dB: -6.846078 dB  TrainTime: 31.06s\n",
      "[94/150] TrainLoss: 0.004904  ValLoss: 0.005893  Val RMSE: 0.0768  Val NMSE: 0.2061  Val NMSE_dB: -6.859652 dB  TrainTime: 29.39s\n",
      "[95/150] TrainLoss: 0.004907  ValLoss: 0.005861  Val RMSE: 0.0766  Val NMSE: 0.2054  Val NMSE_dB: -6.874579 dB  TrainTime: 30.84s\n",
      "[96/150] TrainLoss: 0.004884  ValLoss: 0.005975  Val RMSE: 0.0773  Val NMSE: 0.2093  Val NMSE_dB: -6.793031 dB  TrainTime: 29.66s\n",
      "[97/150] TrainLoss: 0.004895  ValLoss: 0.005978  Val RMSE: 0.0773  Val NMSE: 0.2097  Val NMSE_dB: -6.784158 dB  TrainTime: 30.00s\n",
      "[98/150] TrainLoss: 0.004942  ValLoss: 0.005946  Val RMSE: 0.0771  Val NMSE: 0.2082  Val NMSE_dB: -6.816068 dB  TrainTime: 30.08s\n",
      "[99/150] TrainLoss: 0.004851  ValLoss: 0.005947  Val RMSE: 0.0771  Val NMSE: 0.2078  Val NMSE_dB: -6.824086 dB  TrainTime: 30.10s\n",
      "[100/150] TrainLoss: 0.004865  ValLoss: 0.005998  Val RMSE: 0.0774  Val NMSE: 0.2104  Val NMSE_dB: -6.770401 dB  TrainTime: 29.10s\n",
      "[101/150] TrainLoss: 0.004866  ValLoss: 0.005990  Val RMSE: 0.0774  Val NMSE: 0.2099  Val NMSE_dB: -6.779805 dB  TrainTime: 29.56s\n",
      "[102/150] TrainLoss: 0.004873  ValLoss: 0.005964  Val RMSE: 0.0772  Val NMSE: 0.2090  Val NMSE_dB: -6.799152 dB  TrainTime: 31.95s\n",
      "[103/150] TrainLoss: 0.004848  ValLoss: 0.005964  Val RMSE: 0.0772  Val NMSE: 0.2084  Val NMSE_dB: -6.810631 dB  TrainTime: 29.74s\n",
      "[104/150] TrainLoss: 0.004866  ValLoss: 0.006081  Val RMSE: 0.0780  Val NMSE: 0.2139  Val NMSE_dB: -6.698442 dB  TrainTime: 29.42s\n",
      "[105/150] TrainLoss: 0.004876  ValLoss: 0.005996  Val RMSE: 0.0774  Val NMSE: 0.2101  Val NMSE_dB: -6.776405 dB  TrainTime: 29.81s\n",
      "[106/150] TrainLoss: 0.004878  ValLoss: 0.005898  Val RMSE: 0.0768  Val NMSE: 0.2065  Val NMSE_dB: -6.850107 dB  TrainTime: 29.57s\n",
      "[107/150] TrainLoss: 0.004841  ValLoss: 0.005895  Val RMSE: 0.0768  Val NMSE: 0.2064  Val NMSE_dB: -6.853780 dB  TrainTime: 28.68s\n",
      "[108/150] TrainLoss: 0.004817  ValLoss: 0.005879  Val RMSE: 0.0767  Val NMSE: 0.2054  Val NMSE_dB: -6.873065 dB  TrainTime: 28.87s\n",
      "[109/150] TrainLoss: 0.004844  ValLoss: 0.005898  Val RMSE: 0.0768  Val NMSE: 0.2061  Val NMSE_dB: -6.858618 dB  TrainTime: 28.71s\n",
      "[110/150] TrainLoss: 0.004830  ValLoss: 0.006013  Val RMSE: 0.0775  Val NMSE: 0.2105  Val NMSE_dB: -6.768138 dB  TrainTime: 28.99s\n",
      "[111/150] TrainLoss: 0.004815  ValLoss: 0.006069  Val RMSE: 0.0779  Val NMSE: 0.2125  Val NMSE_dB: -6.726855 dB  TrainTime: 29.40s\n",
      "[112/150] TrainLoss: 0.004828  ValLoss: 0.005990  Val RMSE: 0.0774  Val NMSE: 0.2101  Val NMSE_dB: -6.775567 dB  TrainTime: 30.04s\n",
      "[113/150] TrainLoss: 0.004806  ValLoss: 0.005985  Val RMSE: 0.0774  Val NMSE: 0.2099  Val NMSE_dB: -6.780765 dB  TrainTime: 30.24s\n",
      "[114/150] TrainLoss: 0.004835  ValLoss: 0.005875  Val RMSE: 0.0767  Val NMSE: 0.2057  Val NMSE_dB: -6.867190 dB  TrainTime: 29.46s\n",
      "[115/150] TrainLoss: 0.004776  ValLoss: 0.005992  Val RMSE: 0.0774  Val NMSE: 0.2101  Val NMSE_dB: -6.776768 dB  TrainTime: 30.36s\n",
      "[116/150] TrainLoss: 0.004794  ValLoss: 0.005967  Val RMSE: 0.0772  Val NMSE: 0.2087  Val NMSE_dB: -6.804616 dB  TrainTime: 30.19s\n",
      "[117/150] TrainLoss: 0.004767  ValLoss: 0.005939  Val RMSE: 0.0771  Val NMSE: 0.2076  Val NMSE_dB: -6.826976 dB  TrainTime: 29.75s\n",
      "[118/150] TrainLoss: 0.004801  ValLoss: 0.006039  Val RMSE: 0.0777  Val NMSE: 0.2116  Val NMSE_dB: -6.744479 dB  TrainTime: 29.71s\n",
      "[119/150] TrainLoss: 0.004811  ValLoss: 0.005955  Val RMSE: 0.0772  Val NMSE: 0.2084  Val NMSE_dB: -6.810712 dB  TrainTime: 29.74s\n",
      "[120/150] TrainLoss: 0.004792  ValLoss: 0.005967  Val RMSE: 0.0772  Val NMSE: 0.2090  Val NMSE_dB: -6.799469 dB  TrainTime: 29.28s\n",
      "[121/150] TrainLoss: 0.004801  ValLoss: 0.005916  Val RMSE: 0.0769  Val NMSE: 0.2069  Val NMSE_dB: -6.842862 dB  TrainTime: 29.32s\n",
      "[122/150] TrainLoss: 0.004802  ValLoss: 0.005971  Val RMSE: 0.0773  Val NMSE: 0.2091  Val NMSE_dB: -6.797102 dB  TrainTime: 30.09s\n",
      "[123/150] TrainLoss: 0.004787  ValLoss: 0.006041  Val RMSE: 0.0777  Val NMSE: 0.2115  Val NMSE_dB: -6.747200 dB  TrainTime: 29.92s\n",
      "[124/150] TrainLoss: 0.004786  ValLoss: 0.005952  Val RMSE: 0.0771  Val NMSE: 0.2088  Val NMSE_dB: -6.802911 dB  TrainTime: 29.81s\n",
      "[125/150] TrainLoss: 0.004767  ValLoss: 0.006010  Val RMSE: 0.0775  Val NMSE: 0.2107  Val NMSE_dB: -6.763670 dB  TrainTime: 29.60s\n",
      "[126/150] TrainLoss: 0.004778  ValLoss: 0.005969  Val RMSE: 0.0773  Val NMSE: 0.2088  Val NMSE_dB: -6.802640 dB  TrainTime: 30.14s\n",
      "[127/150] TrainLoss: 0.004751  ValLoss: 0.005966  Val RMSE: 0.0772  Val NMSE: 0.2088  Val NMSE_dB: -6.803348 dB  TrainTime: 28.98s\n",
      "[128/150] TrainLoss: 0.004755  ValLoss: 0.005996  Val RMSE: 0.0774  Val NMSE: 0.2105  Val NMSE_dB: -6.768227 dB  TrainTime: 29.11s\n",
      "[129/150] TrainLoss: 0.004753  ValLoss: 0.005934  Val RMSE: 0.0770  Val NMSE: 0.2078  Val NMSE_dB: -6.823626 dB  TrainTime: 29.31s\n",
      "[130/150] TrainLoss: 0.004727  ValLoss: 0.005962  Val RMSE: 0.0772  Val NMSE: 0.2088  Val NMSE_dB: -6.803561 dB  TrainTime: 29.00s\n",
      "[131/150] TrainLoss: 0.004742  ValLoss: 0.006051  Val RMSE: 0.0778  Val NMSE: 0.2122  Val NMSE_dB: -6.732054 dB  TrainTime: 28.82s\n",
      "[132/150] TrainLoss: 0.004750  ValLoss: 0.006005  Val RMSE: 0.0775  Val NMSE: 0.2107  Val NMSE_dB: -6.763363 dB  TrainTime: 28.66s\n",
      "[133/150] TrainLoss: 0.004777  ValLoss: 0.006084  Val RMSE: 0.0780  Val NMSE: 0.2126  Val NMSE_dB: -6.723980 dB  TrainTime: 28.74s\n",
      "[134/150] TrainLoss: 0.004754  ValLoss: 0.005980  Val RMSE: 0.0773  Val NMSE: 0.2095  Val NMSE_dB: -6.788336 dB  TrainTime: 29.04s\n",
      "[135/150] TrainLoss: 0.004723  ValLoss: 0.006026  Val RMSE: 0.0776  Val NMSE: 0.2110  Val NMSE_dB: -6.757153 dB  TrainTime: 29.03s\n",
      "[136/150] TrainLoss: 0.004744  ValLoss: 0.006021  Val RMSE: 0.0776  Val NMSE: 0.2106  Val NMSE_dB: -6.766400 dB  TrainTime: 29.00s\n",
      "[137/150] TrainLoss: 0.004712  ValLoss: 0.005984  Val RMSE: 0.0774  Val NMSE: 0.2097  Val NMSE_dB: -6.784300 dB  TrainTime: 29.17s\n",
      "[138/150] TrainLoss: 0.004719  ValLoss: 0.005939  Val RMSE: 0.0771  Val NMSE: 0.2081  Val NMSE_dB: -6.816439 dB  TrainTime: 28.81s\n",
      "[139/150] TrainLoss: 0.004720  ValLoss: 0.005949  Val RMSE: 0.0771  Val NMSE: 0.2078  Val NMSE_dB: -6.822974 dB  TrainTime: 30.19s\n",
      "[140/150] TrainLoss: 0.004721  ValLoss: 0.006009  Val RMSE: 0.0775  Val NMSE: 0.2105  Val NMSE_dB: -6.767628 dB  TrainTime: 29.86s\n",
      "[141/150] TrainLoss: 0.004695  ValLoss: 0.006010  Val RMSE: 0.0775  Val NMSE: 0.2106  Val NMSE_dB: -6.765210 dB  TrainTime: 28.83s\n",
      "[142/150] TrainLoss: 0.004694  ValLoss: 0.006032  Val RMSE: 0.0777  Val NMSE: 0.2113  Val NMSE_dB: -6.751817 dB  TrainTime: 29.22s\n",
      "[143/150] TrainLoss: 0.004690  ValLoss: 0.005985  Val RMSE: 0.0774  Val NMSE: 0.2096  Val NMSE_dB: -6.786338 dB  TrainTime: 28.76s\n",
      "[144/150] TrainLoss: 0.004701  ValLoss: 0.005997  Val RMSE: 0.0774  Val NMSE: 0.2094  Val NMSE_dB: -6.789368 dB  TrainTime: 28.69s\n",
      "[145/150] TrainLoss: 0.004685  ValLoss: 0.006021  Val RMSE: 0.0776  Val NMSE: 0.2109  Val NMSE_dB: -6.758936 dB  TrainTime: 29.08s\n",
      "[146/150] TrainLoss: 0.004720  ValLoss: 0.005972  Val RMSE: 0.0773  Val NMSE: 0.2089  Val NMSE_dB: -6.800803 dB  TrainTime: 28.57s\n",
      "[147/150] TrainLoss: 0.004701  ValLoss: 0.006077  Val RMSE: 0.0780  Val NMSE: 0.2131  Val NMSE_dB: -6.714660 dB  TrainTime: 28.87s\n",
      "[148/150] TrainLoss: 0.004707  ValLoss: 0.006057  Val RMSE: 0.0778  Val NMSE: 0.2117  Val NMSE_dB: -6.742061 dB  TrainTime: 28.63s\n",
      "[149/150] TrainLoss: 0.004679  ValLoss: 0.006023  Val RMSE: 0.0776  Val NMSE: 0.2107  Val NMSE_dB: -6.762979 dB  TrainTime: 28.88s\n",
      "[150/150] TrainLoss: 0.004701  ValLoss: 0.005951  Val RMSE: 0.0771  Val NMSE: 0.2084  Val NMSE_dB: -6.811135 dB  TrainTime: 28.84s\n",
      "============================================================\n",
      "=> Best ValLoss   : 0.005791   (epoch 66)\n",
      "=> Best Val NMSE  : 0.2027   (epoch 66)\n",
      "=> Best Val RMSE  : 0.0761   (epoch 66)\n",
      "=> Best Val NMSE_dB: -6.9316 dB (epoch 66)\n",
      "============================================================\n",
      "\n",
      "LWM training log saved to /home/dlghdbs200/LWM_denver/LWM_epoch150_antenna.txt\n"
     ]
    }
   ],
   "source": [
    "import sys, os, contextlib\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "epochs = 150\n",
    "\n",
    "# --- Tee that mirrors stdout to both console and a file ---\n",
    "class Tee:\n",
    "    def __init__(self, filename, mode=\"w\", encoding=\"utf-8\"):\n",
    "        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "        self.file = open(filename, mode, encoding=encoding)\n",
    "        self.stdout = sys.stdout\n",
    "\n",
    "    def write(self, data):\n",
    "        self.file.write(data)\n",
    "        self.stdout.write(data)\n",
    "\n",
    "    def flush(self):\n",
    "        self.file.flush()\n",
    "        self.stdout.flush()\n",
    "\n",
    "    def close(self):\n",
    "        self.file.close()\n",
    "\n",
    "    # context manager support\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc, tb):\n",
    "        self.close()\n",
    "        # don't suppress exceptions\n",
    "        return False\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# \n",
    "base_dir = \"/home/dlghdbs200/LWM_denver\"\n",
    "\n",
    "print(\"=== MLP Training ===\")\n",
    "mlp = MLP(hidden=256, depth=3, pdrop=0.1)\n",
    "mlp_path = os.path.join(base_dir, \"MLP_epoch5_antenna.txt\")\n",
    "with Tee(mlp_path, \"w\") as tee:\n",
    "    with contextlib.redirect_stdout(tee):\n",
    "        print(\"=== Training MLP ===\")\n",
    "        run(mlp, epochs=1, lr=1e-3)\n",
    "print(f\"\\nMLP training log saved to {mlp_path}\")\n",
    "\n",
    "print(\"\\n=== CNN Training ===\")\n",
    "cnn = CNN1D()\n",
    "cnn_path = os.path.join(base_dir, \"CNN_epoch5_antenna.txt\")\n",
    "with Tee(cnn_path, \"w\") as tee:\n",
    "    with contextlib.redirect_stdout(tee):\n",
    "        print(\"=== Training CNN ===\")\n",
    "        run(cnn, epochs=1, lr=1e-3)\n",
    "print(f\"\\nCNN training log saved to {cnn_path}\")\n",
    "\n",
    "print(\"\\n=== LWM Training ===\")\n",
    "\n",
    "lwm_model = LWM_denver(d_model=64, n_layers=12)\n",
    "lwm_path = os.path.join(base_dir, \"LWM_epoch150_antenna.txt\")\n",
    "with Tee(lwm_path, \"w\") as tee:\n",
    "    with contextlib.redirect_stdout(tee):\n",
    "        print(\"=== Training LWM ===\")\n",
    "        sample_x, _ = next(iter(train_loader))\n",
    "        L = sample_x.shape[2]\n",
    "        D = sample_x.shape[-1]\n",
    "        lwm_model.build_core(L=L, D=D, device=device)\n",
    "        run(lwm_model, epochs=epochs, lr=1e-3)\n",
    "print(f\"\\nLWM training log saved to {lwm_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bd94bb-9867-4fb0-bbf5-0ac7bc61640d",
   "metadata": {},
   "source": [
    "# inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0010989b-a534-47a5-851a-c3c20c8c2ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472fe423-646b-4c3e-8702-aa781b2dfe95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdcee05-2518-4942-9e49-bab53364fe23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cc8a36-1a56-49bc-8046-405e0ace959f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1715b26a-2eae-49a1-a4ea-180d6b9bd0df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f5d427-e1c9-4587-8bb6-e940675c6252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd0a9e8-f569-4dde-b8f6-10bbe1df38ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ab0b50-e1b5-4bc0-8114-536588543b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096d4922-6e14-401d-9fd5-8c3818cc47ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
